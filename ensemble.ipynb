{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kurisu/anaconda3/envs/2080/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kurisu/anaconda3/envs/2080/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kurisu/anaconda3/envs/2080/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kurisu/anaconda3/envs/2080/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kurisu/anaconda3/envs/2080/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kurisu/anaconda3/envs/2080/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/kurisu/anaconda3/envs/2080/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/kurisu/anaconda3/envs/2080/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/kurisu/anaconda3/envs/2080/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/kurisu/anaconda3/envs/2080/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/kurisu/anaconda3/envs/2080/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/kurisu/anaconda3/envs/2080/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-25 01:39:51,242 Reading data from data\n",
      "2020-03-25 01:39:51,244 Train: data/train.txt\n",
      "2020-03-25 01:39:51,245 Dev: data/valid.txt\n",
      "2020-03-25 01:39:51,246 Test: data/test.txt\n",
      "Corpus: 14041 train + 3250 dev + 3453 test sentences\n",
      "Dictionary with 12 tags: <unk>, O, B-ORG, B-MISC, B-PER, I-PER, B-LOC, I-ORG, I-MISC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "columns = {0: 'text', 1: '_', 2: '_', 3: 'ner'}\n",
    "\n",
    "data_folder = './data'\n",
    "\n",
    "corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                              train_file='train.txt',\n",
    "                              test_file='test.txt',\n",
    "                              dev_file='valid.txt')\n",
    "print(corpus)\n",
    "tag_type = 'ner'\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "print(tag_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Tagger: [\n",
      "SequenceTagger(\n",
      "  (embeddings): ELMoEmbeddings(model=elmo-small)\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (rnn): LSTM(768, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=12, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      "),\n",
      "SequenceTagger(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (model): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "  (rnn): LSTM(3072, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=12, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      "),\n",
      "SequenceTagger(\n",
      "  (embeddings): FlairEmbeddings(\n",
      "    (lm): LanguageModel(\n",
      "      (drop): Dropout(p=0.05, inplace=False)\n",
      "      (encoder): Embedding(300, 100)\n",
      "      (rnn): LSTM(100, 2048)\n",
      "      (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (rnn): LSTM(2048, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=12, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import ELMoEmbeddings,BertEmbeddings,FlairEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from ensemble_tagger import EnsembleTagger\n",
    "from typing import List\n",
    "\n",
    "elmo_tagger = SequenceTagger(hidden_size=256,\n",
    "                             embeddings=ELMoEmbeddings('small'),\n",
    "                             tag_dictionary=tag_dictionary,\n",
    "                             tag_type=tag_type,\n",
    "                             use_crf=True)\n",
    "bert_tagger = SequenceTagger(hidden_size=256,\n",
    "                             embeddings=BertEmbeddings(),\n",
    "                             tag_dictionary=tag_dictionary,\n",
    "                             tag_type=tag_type,\n",
    "                             use_crf=True)\n",
    "flair_tagger = SequenceTagger(hidden_size=256,\n",
    "                              embeddings=FlairEmbeddings('en-forward'),\n",
    "                              tag_dictionary=tag_dictionary,\n",
    "                              tag_type=tag_type,\n",
    "                              use_crf=True)\n",
    "ensemble_tagger = EnsembleTagger(models=[elmo_tagger, bert_tagger, flair_tagger],\n",
    "                                 tag_dictionary=tag_dictionary,\n",
    "                                 tag_type=tag_type,\n",
    "                                 use_crf=True,\n",
    "                                 mode='loss')\n",
    "print(str(ensemble_tagger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-25 01:40:38,818 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:40:38,824 Model: \"Ensemble Tagger: [\n",
      "SequenceTagger(\n",
      "  (embeddings): ELMoEmbeddings(model=elmo-small)\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (rnn): LSTM(768, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=12, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      "),\n",
      "SequenceTagger(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (model): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=3072, out_features=3072, bias=True)\n",
      "  (rnn): LSTM(3072, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=12, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      "),\n",
      "SequenceTagger(\n",
      "  (embeddings): FlairEmbeddings(\n",
      "    (lm): LanguageModel(\n",
      "      (drop): Dropout(p=0.05, inplace=False)\n",
      "      (encoder): Embedding(300, 100)\n",
      "      (rnn): LSTM(100, 2048)\n",
      "      (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (rnn): LSTM(2048, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=12, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\n",
      "]\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-25 01:40:38,825 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:40:38,827 Corpus: \"Corpus: 14041 train + 3250 dev + 3453 test sentences\"\n",
      "2020-03-25 01:40:38,828 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:40:38,830 Parameters:\n",
      "2020-03-25 01:40:38,830  - learning_rate: \"0.01\"\n",
      "2020-03-25 01:40:38,831  - mini_batch_size: \"64\"\n",
      "2020-03-25 01:40:38,833  - patience: \"3\"\n",
      "2020-03-25 01:40:38,834  - anneal_factor: \"0.5\"\n",
      "2020-03-25 01:40:38,835  - max_epochs: \"150\"\n",
      "2020-03-25 01:40:38,835  - shuffle: \"True\"\n",
      "2020-03-25 01:40:38,836  - train_with_dev: \"False\"\n",
      "2020-03-25 01:40:38,837  - batch_growth_annealing: \"False\"\n",
      "2020-03-25 01:40:38,837 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:40:38,838 Model training base path: \"log/ensemble_20200325014038\"\n",
      "2020-03-25 01:40:38,839 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:40:38,840 Device: cuda:0\n",
      "2020-03-25 01:40:38,841 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:40:38,841 Embeddings storage mode: cpu\n",
      "2020-03-25 01:40:38,845 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:40:45,510 epoch 1 - iter 22/220 - loss 20.71888308 - samples/sec: 211.39\n",
      "2020-03-25 01:40:52,305 epoch 1 - iter 44/220 - loss 18.10541762 - samples/sec: 217.26\n",
      "2020-03-25 01:40:59,612 epoch 1 - iter 66/220 - loss 16.73312816 - samples/sec: 201.64\n",
      "2020-03-25 01:41:06,269 epoch 1 - iter 88/220 - loss 15.86788989 - samples/sec: 222.02\n",
      "2020-03-25 01:41:12,993 epoch 1 - iter 110/220 - loss 15.49000178 - samples/sec: 219.70\n",
      "2020-03-25 01:41:19,850 epoch 1 - iter 132/220 - loss 15.15287236 - samples/sec: 215.28\n",
      "2020-03-25 01:41:26,625 epoch 1 - iter 154/220 - loss 14.95629365 - samples/sec: 218.14\n",
      "2020-03-25 01:41:33,458 epoch 1 - iter 176/220 - loss 14.77948161 - samples/sec: 216.04\n",
      "2020-03-25 01:41:40,332 epoch 1 - iter 198/220 - loss 14.59251263 - samples/sec: 214.43\n",
      "2020-03-25 01:41:47,078 epoch 1 - iter 220/220 - loss 14.46673683 - samples/sec: 220.64\n",
      "2020-03-25 01:41:47,384 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:41:47,385 EPOCH 1 done: loss 14.4667 - lr 0.0100\n",
      "2020-03-25 01:41:53,226 DEV : loss 14.279354095458984 - score 0.0\n",
      "2020-03-25 01:41:53,315 BAD EPOCHS (no improvement): 0\n",
      "2020-03-25 01:41:53,356 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:41:59,951 epoch 2 - iter 22/220 - loss 13.67268567 - samples/sec: 213.70\n",
      "2020-03-25 01:42:06,780 epoch 2 - iter 44/220 - loss 13.21522706 - samples/sec: 216.19\n",
      "2020-03-25 01:42:13,571 epoch 2 - iter 66/220 - loss 13.09055882 - samples/sec: 217.35\n",
      "2020-03-25 01:42:20,339 epoch 2 - iter 88/220 - loss 13.10400439 - samples/sec: 218.21\n",
      "2020-03-25 01:42:27,223 epoch 2 - iter 110/220 - loss 13.21875200 - samples/sec: 214.31\n",
      "2020-03-25 01:42:33,980 epoch 2 - iter 132/220 - loss 13.12635690 - samples/sec: 218.28\n",
      "2020-03-25 01:42:41,109 epoch 2 - iter 154/220 - loss 13.12588092 - samples/sec: 206.66\n",
      "2020-03-25 01:42:47,991 epoch 2 - iter 176/220 - loss 13.06299239 - samples/sec: 214.23\n",
      "2020-03-25 01:42:54,937 epoch 2 - iter 198/220 - loss 13.09372667 - samples/sec: 212.51\n",
      "2020-03-25 01:43:00,972 epoch 2 - iter 220/220 - loss 13.11980685 - samples/sec: 245.42\n",
      "2020-03-25 01:43:01,275 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:43:01,276 EPOCH 2 done: loss 13.1198 - lr 0.0100\n",
      "2020-03-25 01:43:07,588 DEV : loss 13.961258888244629 - score 0.0\n",
      "2020-03-25 01:43:07,679 BAD EPOCHS (no improvement): 1\n",
      "2020-03-25 01:43:07,728 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:43:14,269 epoch 3 - iter 22/220 - loss 12.87926154 - samples/sec: 215.45\n",
      "2020-03-25 01:43:20,720 epoch 3 - iter 44/220 - loss 12.87346259 - samples/sec: 229.70\n",
      "2020-03-25 01:43:27,437 epoch 3 - iter 66/220 - loss 12.83987957 - samples/sec: 219.61\n",
      "2020-03-25 01:43:34,263 epoch 3 - iter 88/220 - loss 12.88546974 - samples/sec: 216.13\n",
      "2020-03-25 01:43:41,211 epoch 3 - iter 110/220 - loss 12.84332493 - samples/sec: 212.05\n",
      "2020-03-25 01:43:48,072 epoch 3 - iter 132/220 - loss 12.91663969 - samples/sec: 214.79\n",
      "2020-03-25 01:43:54,976 epoch 3 - iter 154/220 - loss 12.90429564 - samples/sec: 213.74\n",
      "2020-03-25 01:44:01,398 epoch 3 - iter 176/220 - loss 12.89656564 - samples/sec: 230.28\n",
      "2020-03-25 01:44:07,632 epoch 3 - iter 198/220 - loss 12.91551129 - samples/sec: 237.05\n",
      "2020-03-25 01:44:14,364 epoch 3 - iter 220/220 - loss 12.91185702 - samples/sec: 219.08\n",
      "2020-03-25 01:44:14,658 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:44:14,659 EPOCH 3 done: loss 12.9119 - lr 0.0100\n",
      "2020-03-25 01:44:20,256 DEV : loss 13.663290977478027 - score 0.0\n",
      "2020-03-25 01:44:20,346 BAD EPOCHS (no improvement): 2\n",
      "2020-03-25 01:44:20,393 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:44:27,135 epoch 4 - iter 22/220 - loss 12.81175262 - samples/sec: 209.02\n",
      "2020-03-25 01:44:33,911 epoch 4 - iter 44/220 - loss 12.61796060 - samples/sec: 217.53\n",
      "2020-03-25 01:44:40,826 epoch 4 - iter 66/220 - loss 12.65433323 - samples/sec: 213.02\n",
      "2020-03-25 01:44:47,000 epoch 4 - iter 88/220 - loss 12.78250698 - samples/sec: 240.11\n",
      "2020-03-25 01:44:53,826 epoch 4 - iter 110/220 - loss 12.67632996 - samples/sec: 216.18\n",
      "2020-03-25 01:45:00,611 epoch 4 - iter 132/220 - loss 12.67241038 - samples/sec: 217.39\n",
      "2020-03-25 01:45:07,520 epoch 4 - iter 154/220 - loss 12.68365771 - samples/sec: 212.77\n",
      "2020-03-25 01:45:14,297 epoch 4 - iter 176/220 - loss 12.75672132 - samples/sec: 217.59\n",
      "2020-03-25 01:45:21,215 epoch 4 - iter 198/220 - loss 12.76338860 - samples/sec: 213.41\n",
      "2020-03-25 01:45:28,110 epoch 4 - iter 220/220 - loss 12.76955767 - samples/sec: 213.67\n",
      "2020-03-25 01:45:28,405 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:45:28,406 EPOCH 4 done: loss 12.7696 - lr 0.0100\n",
      "2020-03-25 01:45:34,003 DEV : loss 13.463582038879395 - score 0.0\n",
      "2020-03-25 01:45:34,092 BAD EPOCHS (no improvement): 3\n",
      "2020-03-25 01:45:34,140 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:45:40,707 epoch 5 - iter 22/220 - loss 12.91558348 - samples/sec: 214.57\n",
      "2020-03-25 01:45:47,696 epoch 5 - iter 44/220 - loss 12.64733481 - samples/sec: 215.64\n",
      "2020-03-25 01:45:54,420 epoch 5 - iter 66/220 - loss 12.60488673 - samples/sec: 219.74\n",
      "2020-03-25 01:46:01,252 epoch 5 - iter 88/220 - loss 12.65175958 - samples/sec: 215.93\n",
      "2020-03-25 01:46:08,264 epoch 5 - iter 110/220 - loss 12.67030773 - samples/sec: 210.94\n",
      "2020-03-25 01:46:15,908 epoch 5 - iter 132/220 - loss 12.75201751 - samples/sec: 212.47\n",
      "2020-03-25 01:46:22,761 epoch 5 - iter 154/220 - loss 12.67844138 - samples/sec: 215.49\n",
      "2020-03-25 01:46:29,656 epoch 5 - iter 176/220 - loss 12.61110888 - samples/sec: 213.75\n",
      "2020-03-25 01:46:36,594 epoch 5 - iter 198/220 - loss 12.65646549 - samples/sec: 212.94\n",
      "2020-03-25 01:46:43,441 epoch 5 - iter 220/220 - loss 12.67998834 - samples/sec: 215.65\n",
      "2020-03-25 01:46:43,759 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:46:43,760 EPOCH 5 done: loss 12.6800 - lr 0.0100\n",
      "2020-03-25 01:46:50,079 DEV : loss 13.321589469909668 - score 0.0\n",
      "Epoch     5: reducing learning rate of group 0 to 5.0000e-03.\n",
      "2020-03-25 01:46:50,170 BAD EPOCHS (no improvement): 4\n",
      "2020-03-25 01:46:50,219 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:46:56,918 epoch 6 - iter 22/220 - loss 12.83040909 - samples/sec: 210.37\n",
      "2020-03-25 01:47:03,935 epoch 6 - iter 44/220 - loss 12.92653890 - samples/sec: 209.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-25 01:47:10,095 epoch 6 - iter 66/220 - loss 12.90190447 - samples/sec: 240.96\n",
      "2020-03-25 01:47:16,938 epoch 6 - iter 88/220 - loss 12.81019738 - samples/sec: 215.76\n",
      "2020-03-25 01:47:23,796 epoch 6 - iter 110/220 - loss 12.64992247 - samples/sec: 215.40\n",
      "2020-03-25 01:47:30,594 epoch 6 - iter 132/220 - loss 12.59430048 - samples/sec: 216.69\n",
      "2020-03-25 01:47:37,397 epoch 6 - iter 154/220 - loss 12.60528687 - samples/sec: 217.87\n",
      "2020-03-25 01:47:44,213 epoch 6 - iter 176/220 - loss 12.63301843 - samples/sec: 216.53\n",
      "2020-03-25 01:47:51,062 epoch 6 - iter 198/220 - loss 12.62941765 - samples/sec: 217.99\n",
      "2020-03-25 01:47:58,053 epoch 6 - iter 220/220 - loss 12.60491306 - samples/sec: 211.21\n",
      "2020-03-25 01:47:58,374 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:47:58,375 EPOCH 6 done: loss 12.6049 - lr 0.0050\n",
      "2020-03-25 01:48:03,990 DEV : loss 13.233296394348145 - score 0.0\n",
      "2020-03-25 01:48:04,080 BAD EPOCHS (no improvement): 1\n",
      "2020-03-25 01:48:04,122 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:48:10,711 epoch 7 - iter 22/220 - loss 12.69980500 - samples/sec: 213.88\n",
      "2020-03-25 01:48:17,531 epoch 7 - iter 44/220 - loss 12.62036098 - samples/sec: 216.28\n",
      "2020-03-25 01:48:24,378 epoch 7 - iter 66/220 - loss 12.61348664 - samples/sec: 215.80\n",
      "2020-03-25 01:48:31,068 epoch 7 - iter 88/220 - loss 12.55638580 - samples/sec: 220.75\n",
      "2020-03-25 01:48:37,992 epoch 7 - iter 110/220 - loss 12.59673682 - samples/sec: 214.92\n",
      "2020-03-25 01:48:45,095 epoch 7 - iter 132/220 - loss 12.58975621 - samples/sec: 211.03\n",
      "2020-03-25 01:48:51,968 epoch 7 - iter 154/220 - loss 12.53429665 - samples/sec: 215.90\n",
      "2020-03-25 01:48:58,895 epoch 7 - iter 176/220 - loss 12.50937105 - samples/sec: 213.05\n",
      "2020-03-25 01:49:05,976 epoch 7 - iter 198/220 - loss 12.50574315 - samples/sec: 208.03\n",
      "2020-03-25 01:49:12,736 epoch 7 - iter 220/220 - loss 12.57136762 - samples/sec: 218.01\n",
      "2020-03-25 01:49:13,047 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:49:13,048 EPOCH 7 done: loss 12.5714 - lr 0.0050\n",
      "2020-03-25 01:49:18,630 DEV : loss 13.18030071258545 - score 0.0\n",
      "2020-03-25 01:49:18,721 BAD EPOCHS (no improvement): 2\n",
      "2020-03-25 01:49:18,864 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:49:25,522 epoch 8 - iter 22/220 - loss 12.46684573 - samples/sec: 211.68\n",
      "2020-03-25 01:49:31,691 epoch 8 - iter 44/220 - loss 12.75292997 - samples/sec: 240.34\n",
      "2020-03-25 01:49:38,535 epoch 8 - iter 66/220 - loss 12.78700370 - samples/sec: 215.87\n",
      "2020-03-25 01:49:45,410 epoch 8 - iter 88/220 - loss 12.72417849 - samples/sec: 216.89\n",
      "2020-03-25 01:49:52,150 epoch 8 - iter 110/220 - loss 12.63533669 - samples/sec: 218.79\n",
      "2020-03-25 01:49:59,078 epoch 8 - iter 132/220 - loss 12.55646881 - samples/sec: 212.81\n",
      "2020-03-25 01:50:06,189 epoch 8 - iter 154/220 - loss 12.51932420 - samples/sec: 207.15\n",
      "2020-03-25 01:50:13,051 epoch 8 - iter 176/220 - loss 12.56177088 - samples/sec: 215.13\n",
      "2020-03-25 01:50:19,940 epoch 8 - iter 198/220 - loss 12.54142506 - samples/sec: 214.25\n",
      "2020-03-25 01:50:26,815 epoch 8 - iter 220/220 - loss 12.53174260 - samples/sec: 214.61\n",
      "2020-03-25 01:50:27,138 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:50:27,139 EPOCH 8 done: loss 12.5317 - lr 0.0050\n",
      "2020-03-25 01:50:33,433 DEV : loss 13.129722595214844 - score 0.0\n",
      "2020-03-25 01:50:33,524 BAD EPOCHS (no improvement): 3\n",
      "2020-03-25 01:50:33,572 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:50:40,163 epoch 9 - iter 22/220 - loss 12.34368307 - samples/sec: 213.84\n",
      "2020-03-25 01:50:46,411 epoch 9 - iter 44/220 - loss 12.35387941 - samples/sec: 237.11\n",
      "2020-03-25 01:50:53,086 epoch 9 - iter 66/220 - loss 12.35121158 - samples/sec: 221.57\n",
      "2020-03-25 01:50:59,906 epoch 9 - iter 88/220 - loss 12.37274278 - samples/sec: 216.61\n",
      "2020-03-25 01:51:06,897 epoch 9 - iter 110/220 - loss 12.28294725 - samples/sec: 212.08\n",
      "2020-03-25 01:51:13,725 epoch 9 - iter 132/220 - loss 12.36421496 - samples/sec: 216.76\n",
      "2020-03-25 01:51:20,081 epoch 9 - iter 154/220 - loss 12.44965176 - samples/sec: 233.60\n",
      "2020-03-25 01:51:27,121 epoch 9 - iter 176/220 - loss 12.48591308 - samples/sec: 209.43\n",
      "2020-03-25 01:51:33,993 epoch 9 - iter 198/220 - loss 12.50458141 - samples/sec: 214.70\n",
      "2020-03-25 01:51:40,836 epoch 9 - iter 220/220 - loss 12.50746636 - samples/sec: 215.71\n",
      "2020-03-25 01:51:41,195 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:51:41,196 EPOCH 9 done: loss 12.5075 - lr 0.0050\n",
      "2020-03-25 01:51:46,793 DEV : loss 13.082772254943848 - score 0.0\n",
      "Epoch     9: reducing learning rate of group 0 to 2.5000e-03.\n",
      "2020-03-25 01:51:46,883 BAD EPOCHS (no improvement): 4\n",
      "2020-03-25 01:51:46,930 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:51:53,511 epoch 10 - iter 22/220 - loss 12.23874044 - samples/sec: 214.19\n",
      "2020-03-25 01:52:00,599 epoch 10 - iter 44/220 - loss 12.63161993 - samples/sec: 207.78\n",
      "2020-03-25 01:52:07,547 epoch 10 - iter 66/220 - loss 12.79786564 - samples/sec: 212.00\n",
      "2020-03-25 01:52:14,436 epoch 10 - iter 88/220 - loss 12.73692297 - samples/sec: 214.19\n",
      "2020-03-25 01:52:21,355 epoch 10 - iter 110/220 - loss 12.67347023 - samples/sec: 213.32\n",
      "2020-03-25 01:52:28,260 epoch 10 - iter 132/220 - loss 12.59747560 - samples/sec: 214.24\n",
      "2020-03-25 01:52:34,432 epoch 10 - iter 154/220 - loss 12.59822113 - samples/sec: 240.47\n",
      "2020-03-25 01:52:40,535 epoch 10 - iter 176/220 - loss 12.53293764 - samples/sec: 243.32\n",
      "2020-03-25 01:52:47,413 epoch 10 - iter 198/220 - loss 12.51693714 - samples/sec: 214.64\n",
      "2020-03-25 01:52:54,291 epoch 10 - iter 220/220 - loss 12.48005751 - samples/sec: 214.60\n",
      "2020-03-25 01:52:54,595 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:52:54,597 EPOCH 10 done: loss 12.4801 - lr 0.0025\n",
      "2020-03-25 01:53:00,208 DEV : loss 13.062853813171387 - score 0.0\n",
      "2020-03-25 01:53:00,298 BAD EPOCHS (no improvement): 1\n",
      "2020-03-25 01:53:00,338 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:53:06,851 epoch 11 - iter 22/220 - loss 12.49894381 - samples/sec: 216.37\n",
      "2020-03-25 01:53:13,594 epoch 11 - iter 44/220 - loss 12.38436428 - samples/sec: 218.47\n",
      "2020-03-25 01:53:20,520 epoch 11 - iter 66/220 - loss 12.49676644 - samples/sec: 213.31\n",
      "2020-03-25 01:53:27,484 epoch 11 - iter 88/220 - loss 12.59575904 - samples/sec: 211.79\n",
      "2020-03-25 01:53:34,439 epoch 11 - iter 110/220 - loss 12.57434512 - samples/sec: 211.93\n",
      "2020-03-25 01:53:41,217 epoch 11 - iter 132/220 - loss 12.56518629 - samples/sec: 217.83\n",
      "2020-03-25 01:53:48,058 epoch 11 - iter 154/220 - loss 12.54839531 - samples/sec: 215.81\n",
      "2020-03-25 01:53:54,995 epoch 11 - iter 176/220 - loss 12.50521924 - samples/sec: 214.83\n",
      "2020-03-25 01:54:01,962 epoch 11 - iter 198/220 - loss 12.49784510 - samples/sec: 211.55\n",
      "2020-03-25 01:54:08,994 epoch 11 - iter 220/220 - loss 12.49234953 - samples/sec: 209.51\n",
      "2020-03-25 01:54:09,304 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:54:09,305 EPOCH 11 done: loss 12.4923 - lr 0.0025\n",
      "2020-03-25 01:54:15,609 DEV : loss 13.04861068725586 - score 0.0\n",
      "2020-03-25 01:54:15,699 BAD EPOCHS (no improvement): 2\n",
      "2020-03-25 01:54:15,747 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:54:22,376 epoch 12 - iter 22/220 - loss 12.43018207 - samples/sec: 212.61\n",
      "2020-03-25 01:54:29,410 epoch 12 - iter 44/220 - loss 12.58253288 - samples/sec: 209.40\n",
      "2020-03-25 01:54:36,387 epoch 12 - iter 66/220 - loss 12.52092574 - samples/sec: 211.53\n",
      "2020-03-25 01:54:43,552 epoch 12 - iter 88/220 - loss 12.49238279 - samples/sec: 205.66\n",
      "2020-03-25 01:54:50,358 epoch 12 - iter 110/220 - loss 12.48027606 - samples/sec: 217.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-25 01:54:56,522 epoch 12 - iter 132/220 - loss 12.43935035 - samples/sec: 240.58\n",
      "2020-03-25 01:55:03,409 epoch 12 - iter 154/220 - loss 12.44124749 - samples/sec: 214.22\n",
      "2020-03-25 01:55:10,244 epoch 12 - iter 176/220 - loss 12.47518829 - samples/sec: 215.52\n",
      "2020-03-25 01:55:16,491 epoch 12 - iter 198/220 - loss 12.48590815 - samples/sec: 236.98\n",
      "2020-03-25 01:55:23,253 epoch 12 - iter 220/220 - loss 12.48101177 - samples/sec: 218.85\n",
      "2020-03-25 01:55:23,570 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:55:23,571 EPOCH 12 done: loss 12.4810 - lr 0.0025\n",
      "2020-03-25 01:55:29,171 DEV : loss 13.035421371459961 - score 0.0\n",
      "2020-03-25 01:55:29,262 BAD EPOCHS (no improvement): 3\n",
      "2020-03-25 01:55:29,309 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:55:35,888 epoch 13 - iter 22/220 - loss 12.85709125 - samples/sec: 214.22\n",
      "2020-03-25 01:55:42,847 epoch 13 - iter 44/220 - loss 12.51746750 - samples/sec: 211.86\n",
      "2020-03-25 01:55:49,630 epoch 13 - iter 66/220 - loss 12.32780783 - samples/sec: 217.72\n",
      "2020-03-25 01:55:56,632 epoch 13 - iter 88/220 - loss 12.38393967 - samples/sec: 211.05\n",
      "2020-03-25 01:56:03,753 epoch 13 - iter 110/220 - loss 12.40957320 - samples/sec: 208.72\n",
      "2020-03-25 01:56:11,527 epoch 13 - iter 132/220 - loss 12.45357716 - samples/sec: 211.69\n",
      "2020-03-25 01:56:18,529 epoch 13 - iter 154/220 - loss 12.49820382 - samples/sec: 210.37\n",
      "2020-03-25 01:56:25,376 epoch 13 - iter 176/220 - loss 12.48807079 - samples/sec: 215.45\n",
      "2020-03-25 01:56:31,508 epoch 13 - iter 198/220 - loss 12.48326621 - samples/sec: 242.38\n",
      "2020-03-25 01:56:38,266 epoch 13 - iter 220/220 - loss 12.49020945 - samples/sec: 218.94\n",
      "2020-03-25 01:56:38,573 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:56:38,574 EPOCH 13 done: loss 12.4902 - lr 0.0025\n",
      "2020-03-25 01:56:44,859 DEV : loss 13.032927513122559 - score 0.0\n",
      "Epoch    13: reducing learning rate of group 0 to 1.2500e-03.\n",
      "2020-03-25 01:56:44,949 BAD EPOCHS (no improvement): 4\n",
      "2020-03-25 01:56:44,990 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:56:51,529 epoch 14 - iter 22/220 - loss 13.01680201 - samples/sec: 215.51\n",
      "2020-03-25 01:56:58,515 epoch 14 - iter 44/220 - loss 12.45494445 - samples/sec: 211.36\n",
      "2020-03-25 01:57:05,349 epoch 14 - iter 66/220 - loss 12.48476973 - samples/sec: 215.67\n",
      "2020-03-25 01:57:12,340 epoch 14 - iter 88/220 - loss 12.49513407 - samples/sec: 210.50\n",
      "2020-03-25 01:57:19,356 epoch 14 - iter 110/220 - loss 12.44987552 - samples/sec: 209.74\n",
      "2020-03-25 01:57:26,346 epoch 14 - iter 132/220 - loss 12.48908825 - samples/sec: 210.96\n",
      "2020-03-25 01:57:33,243 epoch 14 - iter 154/220 - loss 12.48605428 - samples/sec: 214.23\n",
      "2020-03-25 01:57:40,035 epoch 14 - iter 176/220 - loss 12.48796263 - samples/sec: 217.50\n",
      "2020-03-25 01:57:47,033 epoch 14 - iter 198/220 - loss 12.45679221 - samples/sec: 210.80\n",
      "2020-03-25 01:57:53,924 epoch 14 - iter 220/220 - loss 12.47681405 - samples/sec: 214.18\n",
      "2020-03-25 01:57:54,245 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:57:54,246 EPOCH 14 done: loss 12.4768 - lr 0.0013\n",
      "2020-03-25 01:57:59,861 DEV : loss 13.025691032409668 - score 0.0\n",
      "2020-03-25 01:57:59,951 BAD EPOCHS (no improvement): 1\n",
      "2020-03-25 01:57:59,997 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:58:06,609 epoch 15 - iter 22/220 - loss 12.36475316 - samples/sec: 213.16\n",
      "2020-03-25 01:58:13,480 epoch 15 - iter 44/220 - loss 12.60361182 - samples/sec: 216.22\n",
      "2020-03-25 01:58:20,441 epoch 15 - iter 66/220 - loss 12.60822500 - samples/sec: 212.13\n",
      "2020-03-25 01:58:27,425 epoch 15 - iter 88/220 - loss 12.61205532 - samples/sec: 211.13\n",
      "2020-03-25 01:58:34,418 epoch 15 - iter 110/220 - loss 12.57843390 - samples/sec: 210.82\n",
      "2020-03-25 01:58:41,503 epoch 15 - iter 132/220 - loss 12.52214779 - samples/sec: 208.83\n",
      "2020-03-25 01:58:48,409 epoch 15 - iter 154/220 - loss 12.50100907 - samples/sec: 213.12\n",
      "2020-03-25 01:58:55,203 epoch 15 - iter 176/220 - loss 12.46767487 - samples/sec: 219.65\n",
      "2020-03-25 01:59:02,007 epoch 15 - iter 198/220 - loss 12.44964383 - samples/sec: 216.99\n",
      "2020-03-25 01:59:09,216 epoch 15 - iter 220/220 - loss 12.47074052 - samples/sec: 216.41\n",
      "2020-03-25 01:59:09,530 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:59:09,532 EPOCH 15 done: loss 12.4707 - lr 0.0013\n",
      "2020-03-25 01:59:15,150 DEV : loss 13.020200729370117 - score 0.0\n",
      "2020-03-25 01:59:15,240 BAD EPOCHS (no improvement): 2\n",
      "2020-03-25 01:59:15,287 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 01:59:21,761 epoch 16 - iter 22/220 - loss 12.22527240 - samples/sec: 217.69\n",
      "2020-03-25 01:59:28,684 epoch 16 - iter 44/220 - loss 12.28038868 - samples/sec: 213.12\n",
      "2020-03-25 01:59:35,559 epoch 16 - iter 66/220 - loss 12.25748399 - samples/sec: 214.89\n",
      "2020-03-25 01:59:42,466 epoch 16 - iter 88/220 - loss 12.23336551 - samples/sec: 213.75\n",
      "2020-03-25 01:59:49,440 epoch 16 - iter 110/220 - loss 12.27613324 - samples/sec: 211.72\n",
      "2020-03-25 01:59:56,219 epoch 16 - iter 132/220 - loss 12.39157919 - samples/sec: 217.64\n",
      "2020-03-25 02:00:03,321 epoch 16 - iter 154/220 - loss 12.41347084 - samples/sec: 207.67\n",
      "2020-03-25 02:00:10,189 epoch 16 - iter 176/220 - loss 12.39638712 - samples/sec: 214.68\n",
      "2020-03-25 02:00:17,207 epoch 16 - iter 198/220 - loss 12.42040272 - samples/sec: 209.95\n",
      "2020-03-25 02:00:24,065 epoch 16 - iter 220/220 - loss 12.46565144 - samples/sec: 217.11\n",
      "2020-03-25 02:00:24,382 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:00:24,384 EPOCH 16 done: loss 12.4657 - lr 0.0013\n",
      "2020-03-25 02:00:30,704 DEV : loss 13.0108642578125 - score 0.0\n",
      "2020-03-25 02:00:30,793 BAD EPOCHS (no improvement): 3\n",
      "2020-03-25 02:00:30,841 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:00:37,359 epoch 17 - iter 22/220 - loss 12.66663105 - samples/sec: 216.19\n",
      "2020-03-25 02:00:44,382 epoch 17 - iter 44/220 - loss 12.34576039 - samples/sec: 210.09\n",
      "2020-03-25 02:00:51,311 epoch 17 - iter 66/220 - loss 12.36150632 - samples/sec: 213.01\n",
      "2020-03-25 02:00:58,114 epoch 17 - iter 88/220 - loss 12.30555893 - samples/sec: 216.70\n",
      "2020-03-25 02:01:05,019 epoch 17 - iter 110/220 - loss 12.44700083 - samples/sec: 215.14\n",
      "2020-03-25 02:01:12,004 epoch 17 - iter 132/220 - loss 12.51244889 - samples/sec: 211.08\n",
      "2020-03-25 02:01:18,817 epoch 17 - iter 154/220 - loss 12.55407641 - samples/sec: 216.51\n",
      "2020-03-25 02:01:26,087 epoch 17 - iter 176/220 - loss 12.52681093 - samples/sec: 207.99\n",
      "2020-03-25 02:01:32,967 epoch 17 - iter 198/220 - loss 12.50418797 - samples/sec: 214.85\n",
      "2020-03-25 02:01:39,913 epoch 17 - iter 220/220 - loss 12.46286720 - samples/sec: 212.16\n",
      "2020-03-25 02:01:40,227 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:01:40,228 EPOCH 17 done: loss 12.4629 - lr 0.0013\n",
      "2020-03-25 02:01:45,848 DEV : loss 13.003064155578613 - score 0.0\n",
      "Epoch    17: reducing learning rate of group 0 to 6.2500e-04.\n",
      "2020-03-25 02:01:45,938 BAD EPOCHS (no improvement): 4\n",
      "2020-03-25 02:01:45,984 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:01:52,579 epoch 18 - iter 22/220 - loss 13.03745495 - samples/sec: 213.69\n",
      "2020-03-25 02:01:59,465 epoch 18 - iter 44/220 - loss 12.75407676 - samples/sec: 214.64\n",
      "2020-03-25 02:02:06,327 epoch 18 - iter 66/220 - loss 12.69549247 - samples/sec: 215.05\n",
      "2020-03-25 02:02:13,326 epoch 18 - iter 88/220 - loss 12.57028058 - samples/sec: 210.69\n",
      "2020-03-25 02:02:20,343 epoch 18 - iter 110/220 - loss 12.57412608 - samples/sec: 210.23\n",
      "2020-03-25 02:02:27,208 epoch 18 - iter 132/220 - loss 12.52219174 - samples/sec: 215.02\n",
      "2020-03-25 02:02:34,413 epoch 18 - iter 154/220 - loss 12.49018223 - samples/sec: 206.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-25 02:02:41,419 epoch 18 - iter 176/220 - loss 12.41774207 - samples/sec: 210.69\n",
      "2020-03-25 02:02:47,652 epoch 18 - iter 198/220 - loss 12.45098725 - samples/sec: 238.49\n",
      "2020-03-25 02:02:53,629 epoch 18 - iter 220/220 - loss 12.46266841 - samples/sec: 248.71\n",
      "2020-03-25 02:02:53,944 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:02:53,946 EPOCH 18 done: loss 12.4627 - lr 0.0006\n",
      "2020-03-25 02:02:59,556 DEV : loss 12.99996566772461 - score 0.0\n",
      "2020-03-25 02:02:59,647 BAD EPOCHS (no improvement): 1\n",
      "2020-03-25 02:02:59,686 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:03:06,128 epoch 19 - iter 22/220 - loss 12.06011144 - samples/sec: 218.75\n",
      "2020-03-25 02:03:13,087 epoch 19 - iter 44/220 - loss 12.52722391 - samples/sec: 211.92\n",
      "2020-03-25 02:03:19,997 epoch 19 - iter 66/220 - loss 12.39395517 - samples/sec: 213.72\n",
      "2020-03-25 02:03:26,925 epoch 19 - iter 88/220 - loss 12.52029606 - samples/sec: 213.25\n",
      "2020-03-25 02:03:33,811 epoch 19 - iter 110/220 - loss 12.50321131 - samples/sec: 214.84\n",
      "2020-03-25 02:03:40,749 epoch 19 - iter 132/220 - loss 12.40539432 - samples/sec: 212.63\n",
      "2020-03-25 02:03:47,725 epoch 19 - iter 154/220 - loss 12.41694567 - samples/sec: 210.89\n",
      "2020-03-25 02:03:54,714 epoch 19 - iter 176/220 - loss 12.47663707 - samples/sec: 210.93\n",
      "2020-03-25 02:04:01,579 epoch 19 - iter 198/220 - loss 12.47768279 - samples/sec: 215.19\n",
      "2020-03-25 02:04:07,877 epoch 19 - iter 220/220 - loss 12.45491843 - samples/sec: 235.59\n",
      "2020-03-25 02:04:08,243 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:04:08,245 EPOCH 19 done: loss 12.4549 - lr 0.0006\n",
      "2020-03-25 02:04:14,544 DEV : loss 12.999018669128418 - score 0.0\n",
      "2020-03-25 02:04:14,634 BAD EPOCHS (no improvement): 2\n",
      "2020-03-25 02:04:14,675 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:04:21,368 epoch 20 - iter 22/220 - loss 12.12202033 - samples/sec: 210.56\n",
      "2020-03-25 02:04:28,524 epoch 20 - iter 44/220 - loss 12.46064704 - samples/sec: 205.89\n",
      "2020-03-25 02:04:35,360 epoch 20 - iter 66/220 - loss 12.38179006 - samples/sec: 215.65\n",
      "2020-03-25 02:04:42,255 epoch 20 - iter 88/220 - loss 12.34552428 - samples/sec: 214.47\n",
      "2020-03-25 02:04:49,049 epoch 20 - iter 110/220 - loss 12.29122210 - samples/sec: 219.54\n",
      "2020-03-25 02:04:56,082 epoch 20 - iter 132/220 - loss 12.41495255 - samples/sec: 209.74\n",
      "2020-03-25 02:05:03,025 epoch 20 - iter 154/220 - loss 12.39977528 - samples/sec: 212.62\n",
      "2020-03-25 02:05:09,957 epoch 20 - iter 176/220 - loss 12.39334459 - samples/sec: 212.97\n",
      "2020-03-25 02:05:16,904 epoch 20 - iter 198/220 - loss 12.41682896 - samples/sec: 212.39\n",
      "2020-03-25 02:05:23,624 epoch 20 - iter 220/220 - loss 12.46616973 - samples/sec: 219.90\n",
      "2020-03-25 02:05:23,940 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:05:23,942 EPOCH 20 done: loss 12.4662 - lr 0.0006\n",
      "2020-03-25 02:05:29,567 DEV : loss 12.996458053588867 - score 0.0\n",
      "2020-03-25 02:05:29,657 BAD EPOCHS (no improvement): 3\n",
      "2020-03-25 02:05:29,705 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:05:36,366 epoch 21 - iter 22/220 - loss 13.11849364 - samples/sec: 211.58\n",
      "2020-03-25 02:05:43,263 epoch 21 - iter 44/220 - loss 12.98403887 - samples/sec: 216.07\n",
      "2020-03-25 02:05:50,186 epoch 21 - iter 66/220 - loss 12.55249981 - samples/sec: 213.91\n",
      "2020-03-25 02:05:57,229 epoch 21 - iter 88/220 - loss 12.44707099 - samples/sec: 208.89\n",
      "2020-03-25 02:06:04,091 epoch 21 - iter 110/220 - loss 12.50516705 - samples/sec: 214.88\n",
      "2020-03-25 02:06:11,766 epoch 21 - iter 132/220 - loss 12.41656114 - samples/sec: 215.82\n",
      "2020-03-25 02:06:18,767 epoch 21 - iter 154/220 - loss 12.44522390 - samples/sec: 210.60\n",
      "2020-03-25 02:06:25,051 epoch 21 - iter 176/220 - loss 12.40479300 - samples/sec: 238.17\n",
      "2020-03-25 02:06:31,390 epoch 21 - iter 198/220 - loss 12.43363881 - samples/sec: 233.28\n",
      "2020-03-25 02:06:37,616 epoch 21 - iter 220/220 - loss 12.45063396 - samples/sec: 238.55\n",
      "2020-03-25 02:06:37,933 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:06:37,934 EPOCH 21 done: loss 12.4506 - lr 0.0006\n",
      "2020-03-25 02:06:43,544 DEV : loss 12.991129875183105 - score 0.0\n",
      "Epoch    21: reducing learning rate of group 0 to 3.1250e-04.\n",
      "2020-03-25 02:06:43,634 BAD EPOCHS (no improvement): 4\n",
      "2020-03-25 02:06:43,690 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:06:50,110 epoch 22 - iter 22/220 - loss 12.08048032 - samples/sec: 219.54\n",
      "2020-03-25 02:06:56,952 epoch 22 - iter 44/220 - loss 12.47220163 - samples/sec: 215.95\n",
      "2020-03-25 02:07:04,431 epoch 22 - iter 66/220 - loss 12.39307775 - samples/sec: 196.28\n",
      "2020-03-25 02:07:11,532 epoch 22 - iter 88/220 - loss 12.60524292 - samples/sec: 207.24\n",
      "2020-03-25 02:07:18,507 epoch 22 - iter 110/220 - loss 12.47720057 - samples/sec: 211.76\n",
      "2020-03-25 02:07:25,510 epoch 22 - iter 132/220 - loss 12.44506236 - samples/sec: 210.60\n",
      "2020-03-25 02:07:32,510 epoch 22 - iter 154/220 - loss 12.45557950 - samples/sec: 210.92\n",
      "2020-03-25 02:07:39,470 epoch 22 - iter 176/220 - loss 12.41125607 - samples/sec: 212.29\n",
      "2020-03-25 02:07:46,289 epoch 22 - iter 198/220 - loss 12.42227221 - samples/sec: 217.06\n",
      "2020-03-25 02:07:53,081 epoch 22 - iter 220/220 - loss 12.44624186 - samples/sec: 217.68\n",
      "2020-03-25 02:07:53,377 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:07:53,379 EPOCH 22 done: loss 12.4462 - lr 0.0003\n",
      "2020-03-25 02:07:59,001 DEV : loss 12.99000358581543 - score 0.0\n",
      "2020-03-25 02:07:59,092 BAD EPOCHS (no improvement): 1\n",
      "2020-03-25 02:07:59,139 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:08:05,731 epoch 23 - iter 22/220 - loss 12.45719572 - samples/sec: 213.76\n",
      "2020-03-25 02:08:12,663 epoch 23 - iter 44/220 - loss 12.43960769 - samples/sec: 212.28\n",
      "2020-03-25 02:08:19,400 epoch 23 - iter 66/220 - loss 12.49091744 - samples/sec: 219.25\n",
      "2020-03-25 02:08:26,313 epoch 23 - iter 88/220 - loss 12.44562407 - samples/sec: 213.59\n",
      "2020-03-25 02:08:33,082 epoch 23 - iter 110/220 - loss 12.33255289 - samples/sec: 217.94\n",
      "2020-03-25 02:08:39,394 epoch 23 - iter 132/220 - loss 12.27820787 - samples/sec: 234.52\n",
      "2020-03-25 02:08:46,325 epoch 23 - iter 154/220 - loss 12.32045076 - samples/sec: 213.35\n",
      "2020-03-25 02:08:53,348 epoch 23 - iter 176/220 - loss 12.44139996 - samples/sec: 209.83\n",
      "2020-03-25 02:09:00,282 epoch 23 - iter 198/220 - loss 12.44513324 - samples/sec: 212.94\n",
      "2020-03-25 02:09:06,661 epoch 23 - iter 220/220 - loss 12.45072898 - samples/sec: 234.11\n",
      "2020-03-25 02:09:06,957 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:09:06,959 EPOCH 23 done: loss 12.4507 - lr 0.0003\n",
      "2020-03-25 02:09:12,592 DEV : loss 12.98908805847168 - score 0.0\n",
      "2020-03-25 02:09:12,683 BAD EPOCHS (no improvement): 2\n",
      "2020-03-25 02:09:12,731 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:09:19,280 epoch 24 - iter 22/220 - loss 12.87900114 - samples/sec: 215.21\n",
      "2020-03-25 02:09:26,281 epoch 24 - iter 44/220 - loss 12.74026582 - samples/sec: 210.31\n",
      "2020-03-25 02:09:33,320 epoch 24 - iter 66/220 - loss 12.61856299 - samples/sec: 209.78\n",
      "2020-03-25 02:09:40,114 epoch 24 - iter 88/220 - loss 12.59097498 - samples/sec: 217.71\n",
      "2020-03-25 02:09:47,061 epoch 24 - iter 110/220 - loss 12.58525189 - samples/sec: 212.32\n",
      "2020-03-25 02:09:53,995 epoch 24 - iter 132/220 - loss 12.54460994 - samples/sec: 213.07\n",
      "2020-03-25 02:10:00,925 epoch 24 - iter 154/220 - loss 12.46121091 - samples/sec: 213.27\n",
      "2020-03-25 02:10:07,901 epoch 24 - iter 176/220 - loss 12.42758156 - samples/sec: 214.08\n",
      "2020-03-25 02:10:14,848 epoch 24 - iter 198/220 - loss 12.47119963 - samples/sec: 212.23\n",
      "2020-03-25 02:10:21,868 epoch 24 - iter 220/220 - loss 12.44307032 - samples/sec: 210.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-25 02:10:22,187 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:10:22,188 EPOCH 24 done: loss 12.4431 - lr 0.0003\n",
      "2020-03-25 02:10:28,518 DEV : loss 12.987386703491211 - score 0.0\n",
      "2020-03-25 02:10:28,608 BAD EPOCHS (no improvement): 3\n",
      "2020-03-25 02:10:28,657 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:10:35,133 epoch 25 - iter 22/220 - loss 12.34693722 - samples/sec: 217.61\n",
      "2020-03-25 02:10:41,952 epoch 25 - iter 44/220 - loss 12.44679553 - samples/sec: 216.63\n",
      "2020-03-25 02:10:48,874 epoch 25 - iter 66/220 - loss 12.24048805 - samples/sec: 214.91\n",
      "2020-03-25 02:10:55,843 epoch 25 - iter 88/220 - loss 12.42086354 - samples/sec: 211.86\n",
      "2020-03-25 02:11:02,860 epoch 25 - iter 110/220 - loss 12.45337475 - samples/sec: 210.23\n",
      "2020-03-25 02:11:09,969 epoch 25 - iter 132/220 - loss 12.45133888 - samples/sec: 207.33\n",
      "2020-03-25 02:11:16,841 epoch 25 - iter 154/220 - loss 12.36315271 - samples/sec: 214.50\n",
      "2020-03-25 02:11:23,779 epoch 25 - iter 176/220 - loss 12.36934027 - samples/sec: 212.70\n",
      "2020-03-25 02:11:30,717 epoch 25 - iter 198/220 - loss 12.39628590 - samples/sec: 212.27\n",
      "2020-03-25 02:11:36,900 epoch 25 - iter 220/220 - loss 12.45468558 - samples/sec: 239.80\n",
      "2020-03-25 02:11:37,215 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:11:37,216 EPOCH 25 done: loss 12.4547 - lr 0.0003\n",
      "2020-03-25 02:11:42,850 DEV : loss 12.9856538772583 - score 0.0\n",
      "Epoch    25: reducing learning rate of group 0 to 1.5625e-04.\n",
      "2020-03-25 02:11:42,940 BAD EPOCHS (no improvement): 4\n",
      "2020-03-25 02:11:42,978 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:11:48,972 epoch 26 - iter 22/220 - loss 13.06551474 - samples/sec: 235.10\n",
      "2020-03-25 02:11:56,082 epoch 26 - iter 44/220 - loss 12.51083630 - samples/sec: 207.17\n",
      "2020-03-25 02:12:03,084 epoch 26 - iter 66/220 - loss 12.62563036 - samples/sec: 210.95\n",
      "2020-03-25 02:12:09,886 epoch 26 - iter 88/220 - loss 12.49788842 - samples/sec: 217.26\n",
      "2020-03-25 02:12:16,596 epoch 26 - iter 110/220 - loss 12.41671057 - samples/sec: 219.94\n",
      "2020-03-25 02:12:23,505 epoch 26 - iter 132/220 - loss 12.32826616 - samples/sec: 213.62\n",
      "2020-03-25 02:12:30,468 epoch 26 - iter 154/220 - loss 12.35584862 - samples/sec: 211.52\n",
      "2020-03-25 02:12:37,341 epoch 26 - iter 176/220 - loss 12.39842897 - samples/sec: 214.64\n",
      "2020-03-25 02:12:44,347 epoch 26 - iter 198/220 - loss 12.44926378 - samples/sec: 210.88\n",
      "2020-03-25 02:12:51,297 epoch 26 - iter 220/220 - loss 12.44042862 - samples/sec: 216.15\n",
      "2020-03-25 02:12:51,620 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:12:51,622 EPOCH 26 done: loss 12.4404 - lr 0.0002\n",
      "2020-03-25 02:12:57,263 DEV : loss 12.984942436218262 - score 0.0\n",
      "2020-03-25 02:12:57,354 BAD EPOCHS (no improvement): 1\n",
      "2020-03-25 02:12:57,392 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:13:03,979 epoch 27 - iter 22/220 - loss 12.52192623 - samples/sec: 213.97\n",
      "2020-03-25 02:13:10,938 epoch 27 - iter 44/220 - loss 12.36319626 - samples/sec: 212.05\n",
      "2020-03-25 02:13:18,025 epoch 27 - iter 66/220 - loss 12.37680126 - samples/sec: 207.69\n",
      "2020-03-25 02:13:24,843 epoch 27 - iter 88/220 - loss 12.27977866 - samples/sec: 216.22\n",
      "2020-03-25 02:13:31,589 epoch 27 - iter 110/220 - loss 12.27242459 - samples/sec: 219.27\n",
      "2020-03-25 02:13:37,982 epoch 27 - iter 132/220 - loss 12.46418108 - samples/sec: 231.24\n",
      "2020-03-25 02:13:44,770 epoch 27 - iter 154/220 - loss 12.40768325 - samples/sec: 217.57\n",
      "2020-03-25 02:13:51,931 epoch 27 - iter 176/220 - loss 12.44455309 - samples/sec: 209.27\n",
      "2020-03-25 02:13:58,926 epoch 27 - iter 198/220 - loss 12.45177204 - samples/sec: 210.86\n",
      "2020-03-25 02:14:05,757 epoch 27 - iter 220/220 - loss 12.44828989 - samples/sec: 216.14\n",
      "2020-03-25 02:14:06,063 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:14:06,065 EPOCH 27 done: loss 12.4483 - lr 0.0002\n",
      "2020-03-25 02:14:12,370 DEV : loss 12.984274864196777 - score 0.0\n",
      "2020-03-25 02:14:12,460 BAD EPOCHS (no improvement): 2\n",
      "2020-03-25 02:14:12,508 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:14:19,294 epoch 28 - iter 22/220 - loss 12.84461841 - samples/sec: 207.68\n",
      "2020-03-25 02:14:26,094 epoch 28 - iter 44/220 - loss 12.82053787 - samples/sec: 216.69\n",
      "2020-03-25 02:14:32,858 epoch 28 - iter 66/220 - loss 12.54313999 - samples/sec: 218.18\n",
      "2020-03-25 02:14:39,722 epoch 28 - iter 88/220 - loss 12.58727202 - samples/sec: 215.25\n",
      "2020-03-25 02:14:46,655 epoch 28 - iter 110/220 - loss 12.46928687 - samples/sec: 212.96\n",
      "2020-03-25 02:14:53,551 epoch 28 - iter 132/220 - loss 12.55472141 - samples/sec: 214.07\n",
      "2020-03-25 02:15:00,546 epoch 28 - iter 154/220 - loss 12.54711817 - samples/sec: 213.39\n",
      "2020-03-25 02:15:07,437 epoch 28 - iter 176/220 - loss 12.51248570 - samples/sec: 214.07\n",
      "2020-03-25 02:15:14,477 epoch 28 - iter 198/220 - loss 12.50235626 - samples/sec: 210.46\n",
      "2020-03-25 02:15:21,174 epoch 28 - iter 220/220 - loss 12.45319518 - samples/sec: 220.31\n",
      "2020-03-25 02:15:21,491 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:15:21,493 EPOCH 28 done: loss 12.4532 - lr 0.0002\n",
      "2020-03-25 02:15:27,124 DEV : loss 12.983940124511719 - score 0.0\n",
      "2020-03-25 02:15:27,214 BAD EPOCHS (no improvement): 3\n",
      "2020-03-25 02:15:27,263 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:15:33,826 epoch 29 - iter 22/220 - loss 12.64827455 - samples/sec: 214.73\n",
      "2020-03-25 02:15:40,773 epoch 29 - iter 44/220 - loss 12.36578293 - samples/sec: 212.31\n",
      "2020-03-25 02:15:47,688 epoch 29 - iter 66/220 - loss 12.46062653 - samples/sec: 213.28\n",
      "2020-03-25 02:15:54,740 epoch 29 - iter 88/220 - loss 12.50888587 - samples/sec: 209.28\n",
      "2020-03-25 02:16:01,677 epoch 29 - iter 110/220 - loss 12.49526324 - samples/sec: 213.05\n",
      "2020-03-25 02:16:08,710 epoch 29 - iter 132/220 - loss 12.41642043 - samples/sec: 209.33\n",
      "2020-03-25 02:16:16,397 epoch 29 - iter 154/220 - loss 12.45353701 - samples/sec: 209.51\n",
      "2020-03-25 02:16:23,401 epoch 29 - iter 176/220 - loss 12.42202126 - samples/sec: 210.08\n",
      "2020-03-25 02:16:30,418 epoch 29 - iter 198/220 - loss 12.50030536 - samples/sec: 209.89\n",
      "2020-03-25 02:16:37,091 epoch 29 - iter 220/220 - loss 12.45594612 - samples/sec: 221.45\n",
      "2020-03-25 02:16:37,402 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:16:37,404 EPOCH 29 done: loss 12.4559 - lr 0.0002\n",
      "2020-03-25 02:16:43,013 DEV : loss 12.983667373657227 - score 0.0\n",
      "Epoch    29: reducing learning rate of group 0 to 7.8125e-05.\n",
      "2020-03-25 02:16:43,105 BAD EPOCHS (no improvement): 4\n",
      "2020-03-25 02:16:43,177 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:16:43,178 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:16:43,179 learning rate too small - quitting training!\n",
      "2020-03-25 02:16:43,180 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:16:43,217 ----------------------------------------------------------------------------------------------------\n",
      "2020-03-25 02:16:43,218 Testing using best model ...\n",
      "2020-03-25 02:16:43,219 loading file log/ensemble_20200325014038/best-model.pt\n",
      "2020-03-25 02:16:46,186 0.0\t0.0\t0.0\n",
      "2020-03-25 02:16:46,187 \n",
      "MICRO_AVG: acc 0.0 - f1-score 0.0\n",
      "MACRO_AVG: acc 0.0 - f1-score 0.0\n",
      "LOC        tp: 0 - fp: 0 - fn: 1668 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "MISC       tp: 0 - fp: 0 - fn: 702 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "ORG        tp: 0 - fp: 0 - fn: 1661 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n",
      "PER        tp: 0 - fp: 0 - fn: 1617 - tn: 0 - precision: 0.0000 - recall: 0.0000 - accuracy: 0.0000 - f1-score: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-25 02:16:46,188 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.0,\n",
       " 'dev_score_history': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'train_loss_history': [14.466736828197133,\n",
       "  13.119806853207676,\n",
       "  12.911857024106112,\n",
       "  12.769557666778564,\n",
       "  12.67998833656311,\n",
       "  12.604913061315363,\n",
       "  12.571367623589255,\n",
       "  12.531742598793723,\n",
       "  12.507466359571977,\n",
       "  12.480057512630117,\n",
       "  12.492349529266358,\n",
       "  12.481011772155762,\n",
       "  12.490209449421275,\n",
       "  12.476814046773043,\n",
       "  12.47074051770297,\n",
       "  12.465651442787864,\n",
       "  12.462867199290882,\n",
       "  12.462668414549395,\n",
       "  12.454918432235718,\n",
       "  12.466169734434647,\n",
       "  12.45063395500183,\n",
       "  12.446241855621338,\n",
       "  12.450728984312578,\n",
       "  12.443070324984463,\n",
       "  12.454685575311833,\n",
       "  12.440428621118718,\n",
       "  12.448289888555353,\n",
       "  12.453195177425037,\n",
       "  12.455946124683727],\n",
       " 'dev_loss_history': [tensor(14.2794, device='cuda:0'),\n",
       "  tensor(13.9613, device='cuda:0'),\n",
       "  tensor(13.6633, device='cuda:0'),\n",
       "  tensor(13.4636, device='cuda:0'),\n",
       "  tensor(13.3216, device='cuda:0'),\n",
       "  tensor(13.2333, device='cuda:0'),\n",
       "  tensor(13.1803, device='cuda:0'),\n",
       "  tensor(13.1297, device='cuda:0'),\n",
       "  tensor(13.0828, device='cuda:0'),\n",
       "  tensor(13.0629, device='cuda:0'),\n",
       "  tensor(13.0486, device='cuda:0'),\n",
       "  tensor(13.0354, device='cuda:0'),\n",
       "  tensor(13.0329, device='cuda:0'),\n",
       "  tensor(13.0257, device='cuda:0'),\n",
       "  tensor(13.0202, device='cuda:0'),\n",
       "  tensor(13.0109, device='cuda:0'),\n",
       "  tensor(13.0031, device='cuda:0'),\n",
       "  tensor(13.0000, device='cuda:0'),\n",
       "  tensor(12.9990, device='cuda:0'),\n",
       "  tensor(12.9965, device='cuda:0'),\n",
       "  tensor(12.9911, device='cuda:0'),\n",
       "  tensor(12.9900, device='cuda:0'),\n",
       "  tensor(12.9891, device='cuda:0'),\n",
       "  tensor(12.9874, device='cuda:0'),\n",
       "  tensor(12.9857, device='cuda:0'),\n",
       "  tensor(12.9849, device='cuda:0'),\n",
       "  tensor(12.9843, device='cuda:0'),\n",
       "  tensor(12.9839, device='cuda:0'),\n",
       "  tensor(12.9837, device='cuda:0')]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.trainers import ModelTrainer\n",
    "from datetime import datetime\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(ensemble_tagger, corpus)\n",
    "timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "trainer.train(\"./log/%s_%s/\" % ('ensemble', str(timestamp)),\n",
    "              learning_rate=0.01,\n",
    "              mini_batch_size=64,\n",
    "              max_epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = corpus.test[0]\n",
    "\n",
    "for entity in sentence.get_spans('ner'):\n",
    "    print(entity)\n",
    "\n",
    "for token in sentence.tokens:\n",
    "    print(str(token.get_tag(\"ner\")))\n",
    "    print(str(token.get_tags_proba_dist(\"ner\")))\n",
    "\n",
    "ensemble_tagger.predict(sentence,all_tag_prob=True)\n",
    "\n",
    "for token in sentence.tokens:\n",
    "    print(token.get_tag(\"ner\").value)\n",
    "    print(token.get_tags_proba_dist(\"ner\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = []\n",
    "for sentence in corpus.test:\n",
    "    for token in sentence.tokens:\n",
    "        real.append(token.get_tag(\"ner\").value)\n",
    "        \n",
    "elmo_pred = []\n",
    "bert_pred = []\n",
    "flair_pred = []\n",
    "ensemble_pred = []\n",
    "for sentence in corpus.test:\n",
    "    ensemble_tagger.predict(sentence,all_tag_prob=True)\n",
    "    for token in sentence.tokens:\n",
    "        ensemble_pred.append(token.get_tag(\"ner\").value)\n",
    "    \n",
    "for sentence in corpus.test:\n",
    "    elmo_tagger.predict(sentence,all_tag_prob=True)\n",
    "    for token in sentence.tokens:\n",
    "        elmo_pred.append(token.get_tag(\"ner\").value)\n",
    "    \n",
    "for sentence in corpus.test:\n",
    "    bert_tagger.predict(sentence,all_tag_prob=True)\n",
    "    for token in sentence.tokens:\n",
    "        bert_pred.append(token.get_tag(\"ner\").value)\n",
    "\n",
    "for sentence in corpus.test:\n",
    "    flair_tagger.predict(sentence,all_tag_prob=True)\n",
    "    for token in sentence.tokens:\n",
    "        flair_pred.append(token.get_tag(\"ner\").value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conlleval import evaluate\n",
    "\n",
    "print(evaluate(real, ensemble_pred))\n",
    "print(evaluate(real, elmo_pred))\n",
    "print(evaluate(real, bert_pred))\n",
    "print(evaluate(real, flair_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
