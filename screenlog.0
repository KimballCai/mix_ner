2020-04-01 19:38:10,235 Reading data from data
2020-04-01 19:38:10,235 Train: data/train.txt
2020-04-01 19:38:10,235 Dev: data/valid.txt
2020-04-01 19:38:10,235 Test: data/test.txt
Corpus: 14041 train + 3250 dev + 3453 test sentences
Dictionary with 12 tags: <unk>, O, B-ORG, B-MISC, B-PER, I-PER, B-LOC, I-ORG, I-MISC, I-LOC, <START>, <STOP>
mix_xf
2020-04-01 19:38:36,787 ----------------------------------------------------------------------------------------------------
2020-04-01 19:38:36,788 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): XLNetEmbeddings(
      model=0-xlnet-large-cased
      (model): XLNetModel(
        (word_embedding): Embedding(32000, 1024)
        (layer): ModuleList(
          (0): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (3): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (4): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (5): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (6): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (7): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (8): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (9): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (10): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (11): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (12): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (13): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (14): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (15): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (16): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (17): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (18): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (19): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (20): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (21): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (22): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (23): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (list_embedding_1): PooledFlairEmbeddings(
      (context_embeddings): FlairEmbeddings(
        (lm): LanguageModel(
          (drop): Dropout(p=0.05, inplace=False)
          (encoder): Embedding(300, 100)
          (rnn): LSTM(100, 2048)
          (decoder): Linear(in_features=2048, out_features=300, bias=True)
        )
      )
    )
    (list_embedding_2): PooledFlairEmbeddings(
      (context_embeddings): FlairEmbeddings(
        (lm): LanguageModel(
          (drop): Dropout(p=0.05, inplace=False)
          (encoder): Embedding(300, 100)
          (rnn): LSTM(100, 2048)
          (decoder): Linear(in_features=2048, out_features=300, bias=True)
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=10240, out_features=10240, bias=True)
  (rnn): LSTM(10240, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=12, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2020-04-01 19:38:36,789 ----------------------------------------------------------------------------------------------------
2020-04-01 19:38:36,789 Corpus: "Corpus: 14041 train + 3250 dev + 3453 test sentences"
2020-04-01 19:38:36,789 ----------------------------------------------------------------------------------------------------
2020-04-01 19:38:36,789 Parameters:
2020-04-01 19:38:36,789  - learning_rate: "0.01"
2020-04-01 19:38:36,789  - mini_batch_size: "64"
2020-04-01 19:38:36,789  - patience: "3"
2020-04-01 19:38:36,789  - anneal_factor: "0.5"
2020-04-01 19:38:36,789  - max_epochs: "150"
2020-04-01 19:38:36,789  - shuffle: "True"
2020-04-01 19:38:36,789  - train_with_dev: "False"
2020-04-01 19:38:36,789  - batch_growth_annealing: "False"
2020-04-01 19:38:36,789 ----------------------------------------------------------------------------------------------------
2020-04-01 19:38:36,789 Model training base path: "log/mix_xf_20200401193836"
2020-04-01 19:38:36,789 ----------------------------------------------------------------------------------------------------
2020-04-01 19:38:36,789 Device: cpu
2020-04-01 19:38:36,789 ----------------------------------------------------------------------------------------------------
2020-04-01 19:38:36,789 Embeddings storage mode: cpu
2020-04-01 19:38:36,792 ----------------------------------------------------------------------------------------------------
train mode resetting embeddings
train mode resetting embeddings
2020-04-01 19:45:16,384 epoch 1 - iter 22/220 - loss 28.82727359 - samples/sec: 3.52
2020-04-01 19:52:23,846 epoch 1 - iter 44/220 - loss 19.43761673 - samples/sec: 3.60
2020-04-01 19:59:32,163 epoch 1 - iter 66/220 - loss 15.53855290 - samples/sec: 3.61
^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^C2020-04-01 20:05:18,050 ----------------------------------------------------------------------------------------------------
2020-04-01 20:05:18,050 Exiting from training early.
2020-04-01 20:05:18,050 Saving model ...
^C^C^C^C^CTraceback (most recent call last):
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/flair/trainers/trainer.py", line 331, in train
    loss = self.model.forward_loss(batch_step)
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py", line 493, in forward_loss
    features = self.forward(data_points)
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py", line 498, in forward
    self.embeddings.embed(sentences)
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/flair/embeddings.py", line 177, in embed
    embedding.embed(sentences)
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/flair/embeddings.py", line 96, in embed
    self._add_embeddings_internal(sentences)
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/flair/embeddings.py", line 1322, in _add_embeddings_internal
    eos_token="</s>",
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/flair/embeddings.py", line 1188, in _get_transformer_sentence_embeddings
    hidden_states = model(tokens_tensor)[-1]
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/transformers/modeling_xlnet.py", line 882, in forward
    head_mask=head_mask[i],
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/transformers/modeling_xlnet.py", line 444, in forward
    head_mask=head_mask,
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/transformers/modeling_xlnet.py", line 379, in forward
    v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/torch/functional.py", line 241, in einsum
    return torch._C._VariableFunctions.einsum(equation, operands)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/torch/serialization.py", line 328, in save
    _legacy_save(obj, opened_file, pickle_module, pickle_protocol)
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/torch/serialization.py", line 407, in _legacy_save
    serialized_storages[key]._write_file(f, _should_read_directly(f))
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 112, in <module>
    max_epochs=150)
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/flair/trainers/trainer.py", line 548, in train
    self.model.save(base_path / "final-model.pt")
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/flair/nn.py", line 70, in save
    torch.save(model_state, str(model_file), pickle_protocol=4)
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/torch/serialization.py", line 328, in save
    _legacy_save(obj, opened_file, pickle_module, pickle_protocol)
  File "/home/qingpeng/anaconda3/envs/py/lib/python3.6/site-packages/torch/serialization.py", line 196, in __exit__
    self.file_like.close()
KeyboardInterrupt
2020-04-01 20:41:21,454 Reading data from data
2020-04-01 20:41:21,454 Train: data/train.txt
2020-04-01 20:41:21,454 Dev: data/valid.txt
2020-04-01 20:41:21,454 Test: data/test.txt
Corpus: 14041 train + 3250 dev + 3453 test sentences
Dictionary with 12 tags: <unk>, O, B-ORG, B-MISC, B-PER, I-PER, B-LOC, I-ORG, I-MISC, I-LOC, <START>, <STOP>
mix_xfe
2020-04-01 20:41:36,161 ----------------------------------------------------------------------------------------------------
2020-04-01 20:41:36,161 ATTENTION! The library "allennlp" is not installed!
2020-04-01 20:41:36,161 To use ELMoEmbeddings, please first install with "pip install allennlp"
2020-04-01 20:41:36,161 ----------------------------------------------------------------------------------------------------
Traceback (most recent call last):
  File "train.py", line 82, in <module>
    ELMoEmbeddings("small")
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/embeddings.py", line 802, in __init__
    options_file = allennlp.commands.elmo.DEFAULT_OPTIONS_FILE
UnboundLocalError: local variable 'allennlp' referenced before assignment
2020-04-01 20:45:22,066 Reading data from data
2020-04-01 20:45:22,066 Train: data/train.txt
2020-04-01 20:45:22,066 Dev: data/valid.txt
2020-04-01 20:45:22,066 Test: data/test.txt
Corpus: 14041 train + 3250 dev + 3453 test sentences
Dictionary with 12 tags: <unk>, O, B-ORG, B-MISC, B-PER, I-PER, B-LOC, I-ORG, I-MISC, I-LOC, <START>, <STOP>
mix_xfe
2020-04-01 20:45:53,971 ----------------------------------------------------------------------------------------------------
2020-04-01 20:45:53,973 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): XLNetEmbeddings(
      model=0-xlnet-large-cased
      (model): XLNetModel(
        (word_embedding): Embedding(32000, 1024)
        (layer): ModuleList(
          (0): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (3): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (4): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (5): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (6): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (7): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (8): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (9): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (10): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (11): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (12): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (13): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (14): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (15): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (16): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (17): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (18): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (19): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (20): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (21): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (22): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (23): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (list_embedding_1): PooledFlairEmbeddings(
      (context_embeddings): FlairEmbeddings(
        (lm): LanguageModel(
          (drop): Dropout(p=0.05, inplace=False)
          (encoder): Embedding(300, 100)
          (rnn): LSTM(100, 2048)
          (decoder): Linear(in_features=2048, out_features=300, bias=True)
        )
      )
    )
    (list_embedding_2): PooledFlairEmbeddings(
      (context_embeddings): FlairEmbeddings(
        (lm): LanguageModel(
          (drop): Dropout(p=0.05, inplace=False)
          (encoder): Embedding(300, 100)
          (rnn): LSTM(100, 2048)
          (decoder): Linear(in_features=2048, out_features=300, bias=True)
        )
      )
    )
    (list_embedding_3): ELMoEmbeddings(model=3-elmo-small)
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=11008, out_features=11008, bias=True)
  (rnn): LSTM(11008, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=12, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2020-04-01 20:45:53,973 ----------------------------------------------------------------------------------------------------
2020-04-01 20:45:53,973 Corpus: "Corpus: 14041 train + 3250 dev + 3453 test sentences"
2020-04-01 20:45:53,973 ----------------------------------------------------------------------------------------------------
2020-04-01 20:45:53,973 Parameters:
2020-04-01 20:45:53,973  - learning_rate: "0.01"
2020-04-01 20:45:53,973  - mini_batch_size: "64"
2020-04-01 20:45:53,973  - patience: "3"
2020-04-01 20:45:53,973  - anneal_factor: "0.5"
2020-04-01 20:45:53,974  - max_epochs: "150"
2020-04-01 20:45:53,974  - shuffle: "True"
2020-04-01 20:45:53,974  - train_with_dev: "False"
2020-04-01 20:45:53,974  - batch_growth_annealing: "False"
2020-04-01 20:45:53,974 ----------------------------------------------------------------------------------------------------
2020-04-01 20:45:53,974 Model training base path: "log/mix_xfe_20200401204553_256"
2020-04-01 20:45:53,974 ----------------------------------------------------------------------------------------------------
2020-04-01 20:45:53,974 Device: cuda:0
2020-04-01 20:45:53,974 ----------------------------------------------------------------------------------------------------
2020-04-01 20:45:53,974 Embeddings storage mode: cpu
2020-04-01 20:45:53,976 ----------------------------------------------------------------------------------------------------
train mode resetting embeddings
train mode resetting embeddings
2020-04-01 20:46:59,647 epoch 1 - iter 22/220 - loss 23.05279029 - samples/sec: 21.44
2020-04-01 20:48:46,997 epoch 1 - iter 44/220 - loss 15.97472772 - samples/sec: 21.23
2020-04-01 20:52:33,877 epoch 1 - iter 66/220 - loss 12.90668929 - samples/sec: 21.21
2020-04-01 20:56:08,020 epoch 1 - iter 88/220 - loss 11.01309988 - samples/sec: 21.25
 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

204 packages can be updated.
136 updates are security updates.

New release '18.04.4 LTS' available.
Run 'do-release-upgrade' to upgrade to it.

Traceback (most recent call last):
  File "train.py", line 16, in <module>
    from flair.data import Corpus
ModuleNotFoundError: No module named 'flair'
Traceback (most recent call last):
  File "train.py", line 16, in <module>
    from flair.data import Corpus
ModuleNotFoundError: No module named 'flair'
2020-04-01 20:59:12,210 Reading data from data
2020-04-01 20:59:12,210 Train: data/train.txt
2020-04-01 20:59:12,210 Dev: data/valid.txt
2020-04-01 20:59:12,210 Test: data/test.txt
Corpus: 14041 train + 3250 dev + 3453 test sentences
Dictionary with 12 tags: <unk>, O, B-ORG, B-MISC, B-PER, I-PER, B-LOC, I-ORG, I-MISC, I-LOC, <START>, <STOP>
mix_xfeb
2020-04-01 20:59:28,919 epoch 1 - iter 110/220 - loss 9.72475253 - samples/sec: 20.86
2020-04-01 21:01:02,719 ----------------------------------------------------------------------------------------------------
2020-04-01 21:01:02,722 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): XLNetEmbeddings(
      model=0-xlnet-large-cased
      (model): XLNetModel(
        (word_embedding): Embedding(32000, 1024)
        (layer): ModuleList(
          (0): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (3): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (4): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (5): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (6): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (7): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (8): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (9): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (10): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (11): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (12): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (13): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (14): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (15): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (16): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (17): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (18): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (19): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (20): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (21): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (22): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (23): XLNetLayer(
            (rel_attn): XLNetRelativeAttention(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (ff): XLNetFeedForward(
              (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
              (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
              (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (list_embedding_1): PooledFlairEmbeddings(
      (context_embeddings): FlairEmbeddings(
        (lm): LanguageModel(
          (drop): Dropout(p=0.05, inplace=False)
          (encoder): Embedding(300, 100)
          (rnn): LSTM(100, 2048)
          (decoder): Linear(in_features=2048, out_features=300, bias=True)
        )
      )
    )
    (list_embedding_2): PooledFlairEmbeddings(
      (context_embeddings): FlairEmbeddings(
        (lm): LanguageModel(
          (drop): Dropout(p=0.05, inplace=False)
          (encoder): Embedding(300, 100)
          (rnn): LSTM(100, 2048)
          (decoder): Linear(in_features=2048, out_features=300, bias=True)
        )
      )
    )
    (list_embedding_3): ELMoEmbeddings(model=3-elmo-small)
    (list_embedding_4): BertEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(30522, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=14080, out_features=14080, bias=True)
  (rnn): LSTM(14080, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=12, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2020-04-01 21:01:02,723 ----------------------------------------------------------------------------------------------------
2020-04-01 21:01:02,723 Corpus: "Corpus: 14041 train + 3250 dev + 3453 test sentences"
2020-04-01 21:01:02,723 ----------------------------------------------------------------------------------------------------
2020-04-01 21:01:02,723 Parameters:
2020-04-01 21:01:02,723  - learning_rate: "0.01"
2020-04-01 21:01:02,723  - mini_batch_size: "32"
2020-04-01 21:01:02,723  - patience: "3"
2020-04-01 21:01:02,723  - anneal_factor: "0.5"
2020-04-01 21:01:02,724  - max_epochs: "150"
2020-04-01 21:01:02,724  - shuffle: "True"
2020-04-01 21:01:02,724  - train_with_dev: "False"
2020-04-01 21:01:02,724  - batch_growth_annealing: "False"
2020-04-01 21:01:02,724 ----------------------------------------------------------------------------------------------------
2020-04-01 21:01:02,724 Model training base path: "log/mix_xfeb_20200401210102_256"
2020-04-01 21:01:02,724 ----------------------------------------------------------------------------------------------------
2020-04-01 21:01:02,724 Device: cuda:0
2020-04-01 21:01:02,724 ----------------------------------------------------------------------------------------------------
2020-04-01 21:01:02,724 Embeddings storage mode: cpu
2020-04-01 21:01:02,727 ----------------------------------------------------------------------------------------------------
train mode resetting embeddings
train mode resetting embeddings
2020-04-01 21:03:05,985 epoch 1 - iter 132/220 - loss 8.74485291 - samples/sec: 20.81
^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B2020-04-01 21:04:09,235 epoch 1 - iter 43/439 - loss 12.50606582 - samples/sec: 7.38
2020-04-01 21:07:30,790 epoch 1 - iter 154/220 - loss 8.02713283 - samples/sec: 20.97
2020-04-01 21:11:59,398 epoch 1 - iter 86/439 - loss 9.44572608 - samples/sec: 7.87

2020-04-01 21:16:17,781 epoch 1 - iter 198/220 - loss 6.96563075 - samples/sec: 21.59
2020-04-01 21:19:11,757 epoch 1 - iter 129/439 - loss 7.90453832 - samples/sec: 7.39
Traceback (most recent call last):
  File "train.py", line 23, in <module>
    ARGS = parse_args()
  File "train.py", line 16, in parse_args
    arg_parser = argparse.ArgumentParser()
NameError: name 'argparse' is not defined
2020-04-01 21:19:40,198 epoch 1 - iter 220/220 - loss 6.57351060 - samples/sec: 21.57
Traceback (most recent call last):
  File "train.py", line 23, in <module>
    ARGS = parse_args()
  File "train.py", line 16, in parse_args
    arg_parser = argparse.ArgumentParser()
NameError: name 'argparse' is not defined
2020-04-01 21:20:33,972 Reading data from data
2020-04-01 21:20:33,973 Train: data/train.txt
2020-04-01 21:20:33,973 Dev: data/valid.txt
2020-04-01 21:20:33,973 Test: data/test.txt
Corpus: 14041 train + 3250 dev + 3453 test sentences
Dictionary with 12 tags: <unk>, O, B-ORG, B-MISC, B-PER, I-PER, B-LOC, I-ORG, I-MISC, I-LOC, <START>, <STOP>
elmo_s
2020-04-01 21:21:03,043 ----------------------------------------------------------------------------------------------------
2020-04-01 21:21:03,043 Model: "SequenceTagger(
  (embeddings): ELMoEmbeddings(model=elmo-small)
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)
  (rnn): LSTM(768, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=12, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2020-04-01 21:21:03,043 ----------------------------------------------------------------------------------------------------
2020-04-01 21:21:03,043 Corpus: "Corpus: 14041 train + 3250 dev + 3453 test sentences"
2020-04-01 21:21:03,043 ----------------------------------------------------------------------------------------------------
2020-04-01 21:21:03,043 Parameters:
2020-04-01 21:21:03,043  - learning_rate: "0.01"
2020-04-01 21:21:03,043  - mini_batch_size: "64"
2020-04-01 21:21:03,043  - patience: "3"
2020-04-01 21:21:03,043  - anneal_factor: "0.5"
2020-04-01 21:21:03,044  - max_epochs: "150"
2020-04-01 21:21:03,044  - shuffle: "True"
2020-04-01 21:21:03,044  - train_with_dev: "False"
2020-04-01 21:21:03,044  - batch_growth_annealing: "False"
2020-04-01 21:21:03,044 ----------------------------------------------------------------------------------------------------
2020-04-01 21:21:03,044 Model training base path: "log/elmo_s_20200401212103_256"
2020-04-01 21:21:03,044 ----------------------------------------------------------------------------------------------------
2020-04-01 21:21:03,044 Device: cuda:0
2020-04-01 21:21:03,044 ----------------------------------------------------------------------------------------------------
2020-04-01 21:21:03,044 Embeddings storage mode: cpu
2020-04-01 21:21:03,046 ----------------------------------------------------------------------------------------------------
2020-04-01 21:21:12,945 epoch 1 - iter 22/220 - loss 35.03582027 - samples/sec: 142.27
2020-04-01 21:21:28,151 epoch 1 - iter 44/220 - loss 23.82255249 - samples/sec: 151.00
2020-04-01 21:21:44,571 epoch 1 - iter 66/220 - loss 18.77589798 - samples/sec: 151.30
2020-04-01 21:22:00,821 epoch 1 - iter 88/220 - loss 15.91463776 - samples/sec: 142.54
2020-04-01 21:22:17,832 epoch 1 - iter 110/220 - loss 13.99874860 - samples/sec: 138.73
2020-04-01 21:22:34,279 epoch 1 - iter 132/220 - loss 12.62927651 - samples/sec: 145.65
2020-04-01 21:22:58,413 ----------------------------------------------------------------------------------------------------
2020-04-01 21:22:58,414 EPOCH 1 done: loss 6.5735 - lr 0.0100
2020-04-01 21:23:07,841 epoch 1 - iter 176/220 - loss 10.74979358 - samples/sec: 145.79
2020-04-01 21:23:17,980 epoch 1 - iter 198/220 - loss 10.01813947 - samples/sec: 177.91
2020-04-01 21:23:32,110 epoch 1 - iter 220/220 - loss 9.42842141 - samples/sec: 166.09
2020-04-01 21:23:37,858 ----------------------------------------------------------------------------------------------------
2020-04-01 21:23:37,858 EPOCH 1 done: loss 9.4284 - lr 0.0100
2020-04-01 21:23:53,583 DEV : loss 3.372437000274658 - score 0.638
2020-04-01 21:23:53,679 BAD EPOCHS (no improvement): 0
2020-04-01 21:23:54,973 ----------------------------------------------------------------------------------------------------
2020-04-01 21:23:58,414 epoch 2 - iter 22/220 - loss 4.00663425 - samples/sec: 409.44
2020-04-01 21:24:07,158 epoch 2 - iter 44/220 - loss 3.96884356 - samples/sec: 410.01
2020-04-01 21:24:11,658 epoch 2 - iter 66/220 - loss 3.85298993 - samples/sec: 422.85
2020-04-01 21:24:16,482 epoch 2 - iter 88/220 - loss 3.73746727 - samples/sec: 417.50
2020-04-01 21:24:18,336 epoch 1 - iter 172/439 - loss 6.91762997 - samples/sec: 18.82
2020-04-01 21:24:26,182 epoch 2 - iter 110/220 - loss 3.67647593 - samples/sec: 329.02
2020-04-01 21:24:36,468 epoch 2 - iter 132/220 - loss 3.64520037 - samples/sec: 307.19
^C2020-04-01 21:24:40,767 ----------------------------------------------------------------------------------------------------
2020-04-01 21:24:40,768 Exiting from training early.
2020-04-01 21:24:40,768 Saving model ...
2020-04-01 21:24:42,070 Done.
2020-04-01 21:24:42,070 ----------------------------------------------------------------------------------------------------
2020-04-01 21:24:42,070 Testing using best model ...
2020-04-01 21:24:42,071 loading file log/elmo_s_20200401212103_256/best-model.pt
^CTraceback (most recent call last):
  File "train.py", line 132, in <module>
    max_epochs=150)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/trainers/trainer.py", line 553, in train
    final_score = self.final_test(base_path, mini_batch_chunk_size, num_workers)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/trainers/trainer.py", line 601, in final_test
    embedding_storage_mode="none",
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py", line 412, in evaluate
    features = self.forward(batch)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py", line 498, in forward
    self.embeddings.embed(sentences)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/embeddings.py", line 96, in embed
    self._add_embeddings_internal(sentences)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/embeddings.py", line 853, in _add_embeddings_internal
    embeddings = self.ee.embed_batch(sentence_words)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/allennlp/commands/elmo.py", line 255, in embed_batch
    embeddings, mask = self.batch_to_embeddings(batch)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/allennlp/commands/elmo.py", line 197, in batch_to_embeddings
    bilm_output = self.elmo_bilm(character_ids)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/allennlp/modules/elmo.py", line 610, in forward
    lstm_outputs = self._elmo_lstm(type_representation, mask)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/allennlp/modules/elmo_lstm.py", line 123, in forward
    self.sort_and_run_forward(self._lstm_forward, inputs, mask)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/allennlp/modules/encoder_base.py", line 116, in sort_and_run_forward
    module_output, final_states = module(packed_sequence_input, initial_states)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/allennlp/modules/elmo_lstm.py", line 217, in _lstm_forward
    forward_state)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/allennlp/modules/lstm_cell_with_projection.py", line 155, in forward
    while batch_lengths[current_length_index] <= index:
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/torch/tensor.py", line 28, in wrapped
    return f(*args, **kwargs)
KeyboardInterrupt
2020-04-01 21:25:19,963 DEV : loss 2.177973985671997 - score 0.7812
2020-04-01 21:25:20,346 BAD EPOCHS (no improvement): 0
2020-04-01 21:26:00,192 ----------------------------------------------------------------------------------------------------
train mode resetting embeddings
train mode resetting embeddings
2020-04-01 21:26:11,804 epoch 2 - iter 22/220 - loss 2.81551353 - samples/sec: 121.96
2020-04-01 21:27:09,532 Reading data from data
2020-04-01 21:27:09,532 Train: data/train.txt
2020-04-01 21:27:09,532 Dev: data/valid.txt
2020-04-01 21:27:09,532 Test: data/test.txt
Corpus: 14041 train + 3250 dev + 3453 test sentences
Dictionary with 12 tags: <unk>, O, B-ORG, B-MISC, B-PER, I-PER, B-LOC, I-ORG, I-MISC, I-LOC, <START>, <STOP>
elmo_s
2020-04-01 21:27:30,587 ----------------------------------------------------------------------------------------------------
2020-04-01 21:27:30,587 Model: "SequenceTagger(
  (embeddings): ELMoEmbeddings(model=elmo-small)
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)
  (rnn): LSTM(768, 64, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=128, out_features=12, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2020-04-01 21:27:30,587 ----------------------------------------------------------------------------------------------------
2020-04-01 21:27:30,587 Corpus: "Corpus: 14041 train + 3250 dev + 3453 test sentences"
2020-04-01 21:27:30,587 ----------------------------------------------------------------------------------------------------
2020-04-01 21:27:30,587 Parameters:
2020-04-01 21:27:30,587  - learning_rate: "0.01"
2020-04-01 21:27:30,588  - mini_batch_size: "64"
2020-04-01 21:27:30,588  - patience: "3"
2020-04-01 21:27:30,588  - anneal_factor: "0.5"
2020-04-01 21:27:30,588  - max_epochs: "150"
2020-04-01 21:27:30,588  - shuffle: "True"
2020-04-01 21:27:30,588  - train_with_dev: "False"
2020-04-01 21:27:30,588  - batch_growth_annealing: "False"
2020-04-01 21:27:30,588 ----------------------------------------------------------------------------------------------------
2020-04-01 21:27:30,588 Model training base path: "log/elmo_s_20200401212730_64"
2020-04-01 21:27:30,588 ----------------------------------------------------------------------------------------------------
2020-04-01 21:27:30,588 Device: cuda:0
2020-04-01 21:27:30,588 ----------------------------------------------------------------------------------------------------
2020-04-01 21:27:30,588 Embeddings storage mode: cpu
2020-04-01 21:27:30,591 ----------------------------------------------------------------------------------------------------
2020-04-01 21:27:40,394 epoch 1 - iter 22/220 - loss 39.79415495 - samples/sec: 143.69
2020-04-01 21:27:55,919 epoch 1 - iter 44/220 - loss 27.52343548 - samples/sec: 149.45
2020-04-01 21:28:12,793 epoch 1 - iter 66/220 - loss 21.38604313 - samples/sec: 147.24
2020-04-01 21:28:28,770 epoch 1 - iter 88/220 - loss 17.96315986 - samples/sec: 158.99
2020-04-01 21:28:44,796 epoch 1 - iter 110/220 - loss 15.66231513 - samples/sec: 156.28
2020-04-01 21:29:00,722 epoch 1 - iter 132/220 - loss 14.01706880 - samples/sec: 153.66
2020-04-01 21:29:15,907 epoch 1 - iter 154/220 - loss 12.81337747 - samples/sec: 169.71
2020-04-01 21:29:30,393 epoch 1 - iter 176/220 - loss 11.88006270 - samples/sec: 170.34
2020-04-01 21:29:44,073 epoch 1 - iter 198/220 - loss 11.08801648 - samples/sec: 170.60
2020-04-01 21:29:41,775 epoch 2 - iter 44/220 - loss 2.66102071 - samples/sec: 118.96
2020-04-01 21:29:59,964 epoch 1 - iter 220/220 - loss 10.44810167 - samples/sec: 157.15
2020-04-01 21:30:10,187 epoch 1 - iter 215/439 - loss 6.22563925 - samples/sec: 18.46
2020-04-01 21:30:06,284 ----------------------------------------------------------------------------------------------------
2020-04-01 21:30:06,284 EPOCH 1 done: loss 10.4481 - lr 0.0100
2020-04-01 21:30:21,984 DEV : loss 3.4431979656219482 - score 0.6137
2020-04-01 21:30:22,080 BAD EPOCHS (no improvement): 0
2020-04-01 21:30:23,250 ----------------------------------------------------------------------------------------------------
2020-04-01 21:30:27,775 epoch 2 - iter 22/220 - loss 4.11335719 - samples/sec: 311.46
2020-04-01 21:30:39,320 epoch 2 - iter 44/220 - loss 4.17356785 - samples/sec: 321.17
2020-04-01 21:30:49,653 epoch 2 - iter 66/220 - loss 4.18907496 - samples/sec: 370.87
2020-04-01 21:30:59,911 epoch 2 - iter 88/220 - loss 4.08926412 - samples/sec: 341.82
2020-04-01 21:31:10,051 epoch 2 - iter 110/220 - loss 4.03015066 - samples/sec: 351.96
2020-04-01 21:31:20,552 epoch 2 - iter 132/220 - loss 3.99759074 - samples/sec: 320.36
2020-04-01 21:31:30,787 epoch 2 - iter 154/220 - loss 3.93516772 - samples/sec: 313.65
2020-04-01 21:31:40,429 epoch 2 - iter 176/220 - loss 3.87047158 - samples/sec: 372.47
2020-04-01 21:31:48,005 Reading data from data
2020-04-01 21:31:48,005 Train: data/train.txt
2020-04-01 21:31:48,005 Dev: data/valid.txt
2020-04-01 21:31:48,005 Test: data/test.txt
Corpus: 14041 train + 3250 dev + 3453 test sentences
Dictionary with 12 tags: <unk>, O, B-ORG, B-MISC, B-PER, I-PER, B-LOC, I-ORG, I-MISC, I-LOC, <START>, <STOP>
elmo_s
2020-04-01 21:31:59,232 epoch 2 - iter 220/220 - loss 3.76740683 - samples/sec: 310.85
2020-04-01 21:32:07,472 ----------------------------------------------------------------------------------------------------
2020-04-01 21:32:07,472 EPOCH 2 done: loss 3.7674 - lr 0.0100
2020-04-01 21:32:11,393 DEV : loss 2.2762773036956787 - score 0.7485
2020-04-01 21:32:11,492 BAD EPOCHS (no improvement): 0
2020-04-01 21:32:12,837 ----------------------------------------------------------------------------------------------------
2020-04-01 21:32:17,832 Reading data from data
2020-04-01 21:32:17,832 Train: data/train.txt
2020-04-01 21:32:17,832 Dev: data/valid.txt
2020-04-01 21:32:17,832 Test: data/test.txt
2020-04-01 21:32:17,286 epoch 3 - iter 22/220 - loss 3.22084894 - samples/sec: 316.72
2020-04-01 21:32:19,140 ----------------------------------------------------------------------------------------------------
2020-04-01 21:32:19,140 Model: "SequenceTagger(
  (embeddings): ELMoEmbeddings(model=elmo-small)
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)
  (rnn): LSTM(768, 128, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=256, out_features=12, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2020-04-01 21:32:19,140 ----------------------------------------------------------------------------------------------------
2020-04-01 21:32:19,140 Corpus: "Corpus: 14041 train + 3250 dev + 3453 test sentences"
2020-04-01 21:32:19,140 ----------------------------------------------------------------------------------------------------
2020-04-01 21:32:19,140 Parameters:
2020-04-01 21:32:19,140  - learning_rate: "0.01"
2020-04-01 21:32:19,141  - mini_batch_size: "32"
2020-04-01 21:32:19,141  - patience: "3"
2020-04-01 21:32:19,141  - anneal_factor: "0.5"
2020-04-01 21:32:19,141  - max_epochs: "150"
2020-04-01 21:32:19,141  - shuffle: "True"
2020-04-01 21:32:19,141  - train_with_dev: "False"
2020-04-01 21:32:19,141  - batch_growth_annealing: "False"
2020-04-01 21:32:19,141 ----------------------------------------------------------------------------------------------------
2020-04-01 21:32:19,141 Model training base path: "log/elmo_s_20200401213219_128"
2020-04-01 21:32:19,141 ----------------------------------------------------------------------------------------------------
2020-04-01 21:32:19,142 Device: cuda:0
2020-04-01 21:32:19,142 ----------------------------------------------------------------------------------------------------
2020-04-01 21:32:19,142 Embeddings storage mode: cpu
2020-04-01 21:32:19,144 ----------------------------------------------------------------------------------------------------
Corpus: 14041 train + 3250 dev + 3453 test sentences
Dictionary with 12 tags: <unk>, O, B-ORG, B-MISC, B-PER, I-PER, B-LOC, I-ORG, I-MISC, I-LOC, <START>, <STOP>
elmo_s
2020-04-01 21:32:35,693 epoch 1 - iter 43/439 - loss 20.21545135 - samples/sec: 83.17
2020-04-01 21:32:37,727 ----------------------------------------------------------------------------------------------------
2020-04-01 21:32:37,727 Model: "SequenceTagger(
  (embeddings): ELMoEmbeddings(model=elmo-small)
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)
  (rnn): LSTM(768, 512, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=1024, out_features=12, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2020-04-01 21:32:37,728 ----------------------------------------------------------------------------------------------------
2020-04-01 21:32:37,728 Corpus: "Corpus: 14041 train + 3250 dev + 3453 test sentences"
2020-04-01 21:32:37,728 ----------------------------------------------------------------------------------------------------
2020-04-01 21:32:37,728 Parameters:
2020-04-01 21:32:37,728  - learning_rate: "0.01"
2020-04-01 21:32:37,728  - mini_batch_size: "32"
2020-04-01 21:32:37,728  - patience: "3"
2020-04-01 21:32:37,728  - anneal_factor: "0.5"
2020-04-01 21:32:37,729  - max_epochs: "150"
2020-04-01 21:32:37,729  - shuffle: "True"
2020-04-01 21:32:37,729  - train_with_dev: "False"
2020-04-01 21:32:37,729  - batch_growth_annealing: "False"
2020-04-01 21:32:37,729 ----------------------------------------------------------------------------------------------------
2020-04-01 21:32:37,729 Model training base path: "log/elmo_s_20200401213237_512"
2020-04-01 21:32:37,729 ----------------------------------------------------------------------------------------------------
2020-04-01 21:32:37,729 Device: cuda:0
2020-04-01 21:32:37,730 ----------------------------------------------------------------------------------------------------
2020-04-01 21:32:37,730 Embeddings storage mode: cpu
2020-04-01 21:32:37,732 ----------------------------------------------------------------------------------------------------
2020-04-01 21:32:38,024 epoch 3 - iter 66/220 - loss 3.27085012 - samples/sec: 327.64
2020-04-01 21:32:54,719 epoch 1 - iter 43/439 - loss 24.18184657 - samples/sec: 81.03
2020-04-01 21:32:58,265 epoch 3 - iter 110/220 - loss 3.15433049 - samples/sec: 293.00
2020-04-01 21:32:58,432 epoch 1 - iter 86/439 - loss 14.54825652 - samples/sec: 84.68
2020-04-01 21:33:10,177 epoch 3 - iter 132/220 - loss 3.12477849 - samples/sec: 312.66
2020-04-01 21:33:18,180 epoch 1 - iter 86/439 - loss 16.89347043 - samples/sec: 87.01
2020-04-01 21:33:19,603 epoch 2 - iter 66/220 - loss 2.62366649 - samples/sec: 116.11
2020-04-01 21:33:20,298 epoch 3 - iter 154/220 - loss 3.06768955 - samples/sec: 322.36
2020-04-01 21:33:32,458 epoch 3 - iter 176/220 - loss 3.03557301 - samples/sec: 304.21
2020-04-01 21:33:42,395 epoch 3 - iter 198/220 - loss 3.00944991 - samples/sec: 322.83
2020-04-01 21:33:41,162 epoch 1 - iter 129/439 - loss 13.57292346 - samples/sec: 88.59
2020-04-01 21:33:53,669 epoch 3 - iter 220/220 - loss 2.98844217 - samples/sec: 335.01
2020-04-01 21:33:59,448 ----------------------------------------------------------------------------------------------------
2020-04-01 21:33:59,448 EPOCH 3 done: loss 2.9884 - lr 0.0100
2020-04-01 21:34:03,348 DEV : loss 1.8400617837905884 - score 0.8083
2020-04-01 21:34:03,442 BAD EPOCHS (no improvement): 0
2020-04-01 21:34:04,745 ----------------------------------------------------------------------------------------------------
2020-04-01 21:34:03,993 epoch 1 - iter 172/439 - loss 11.65512070 - samples/sec: 87.25
2020-04-01 21:34:08,794 epoch 4 - iter 22/220 - loss 2.76261543 - samples/sec: 348.09
2020-04-01 21:34:27,543 epoch 1 - iter 215/439 - loss 10.34302983 - samples/sec: 82.79
2020-04-01 21:34:28,799 epoch 4 - iter 66/220 - loss 2.73243501 - samples/sec: 330.20
2020-04-01 21:34:40,728 epoch 4 - iter 88/220 - loss 2.70961524 - samples/sec: 272.53
2020-04-01 21:34:53,662 epoch 1 - iter 301/439 - loss 7.79467046 - samples/sec: 84.73
2020-04-01 21:34:50,347 epoch 1 - iter 258/439 - loss 9.47112029 - samples/sec: 85.39

2020-04-01 21:35:00,288 epoch 4 - iter 132/220 - loss 2.66820064 - samples/sec: 361.93
2020-04-01 21:35:15,331 epoch 1 - iter 344/439 - loss 7.27884709 - samples/sec: 88.15
2020-04-01 21:35:12,184 epoch 1 - iter 301/439 - loss 8.67228918 - samples/sec: 86.55
2020-04-01 21:35:25,997 epoch 4 - iter 154/220 - loss 2.67381330 - samples/sec: 334.96
2020-04-01 21:35:38,487 epoch 4 - iter 176/220 - loss 2.65466158 - samples/sec: 420.10
2020-04-01 21:35:46,540 epoch 1 - iter 387/439 - loss 6.86759966 - samples/sec: 87.17

2020-04-01 21:35:46,238 epoch 1 - iter 344/439 - loss 8.06112305 - samples/sec: 88.16
2020-04-01 21:35:57,391 epoch 4 - iter 220/220 - loss 2.60977543 - samples/sec: 441.15
2020-04-01 21:36:09,593 epoch 1 - iter 430/439 - loss 6.52380347 - samples/sec: 86.98
-------------------------------------
2020-04-01 21:36:03,258 EPOCH 4 done: loss 2.6098 - lr 0.0100
2020-04-01 21:36:07,052 DEV : loss 1.627373218536377 - score 0.8342
2020-04-01 21:36:07,149 BAD EPOCHS (no improvement): 0
2020-04-01 21:36:08,363 ----------------------------------------------------------------------------------------------------
2020-04-01 21:36:09,277 epoch 1 - iter 387/439 - loss 7.57554916 - samples/sec: 87.10
2020-04-01 21:36:19,939 ----------------------------------------------------------------------------------------------------
2020-04-01 21:36:19,940 EPOCH 1 done: loss 6.4624 - lr 0.0100
2020-04-01 21:36:11,694 epoch 5 - iter 22/220 - loss 2.54594045 - samples/sec: 423.03
2020-04-01 21:36:21,650 epoch 5 - iter 44/220 - loss 2.51522622 - samples/sec: 428.52
2020-04-01 21:36:30,307 epoch 5 - iter 66/220 - loss 2.47072703 - samples/sec: 449.27
2020-04-01 21:36:40,496 epoch 5 - iter 88/220 - loss 2.46736960 - samples/sec: 451.75
0,101 BAD EPOCHS (no improvement): 0
2020-04-01 21:36:40,541 epoch 1 - iter 258/439 - loss 5.68784215 - samples/sec: 17.92
2020-04-01 21:36:31,334 epoch 1 - iter 430/439 - loss 7.17362034 - samples/sec: 90.08
2020-04-01 21:36:41,267 ----------------------------------------------------------------------------------------------------
2020-04-01 21:36:49,555 epoch 2 - iter 43/439 - loss 3.32943107 - samples/sec: 166.13
2020-04-01 21:36:41,680 ----------------------------------------------------------------------------------------------------
2020-04-01 21:36:41,680 EPOCH 1 done: loss 7.1007 - lr 0.0100
2020-04-01 21:37:00,572 epoch 5 - iter 132/220 - loss 2.44656590 - samples/sec: 425.31
2020-04-01 21:37:10,570 epoch 5 - iter 154/220 - loss 2.42678076 - samples/sec: 351.85
2020-04-01 21:37:01,671 DEV : loss 2.477200984954834 - score 0.7229
2020-04-01 21:37:01,766 BAD EPOCHS (no improvement): 0
2020-04-01 21:37:03,313 ----------------------------------------------------------------------------------------------------
2020-04-01 21:37:19,023 epoch 2 - iter 129/439 - loss 3.16603205 - samples/sec: 159.26
2020-04-01 21:37:12,283 epoch 2 - iter 43/439 - loss 3.23970755 - samples/sec: 153.50
2020-04-01 21:37:28,758 epoch 2 - iter 86/439 - loss 3.17399048 - samples/sec: 150.15

2020-04-01 21:37:39,591 epoch 5 - iter 220/220 - loss 2.38171303 - samples/sec: 329.12
2020-04-01 21:37:45,620 ----------------------------------------------------------------------------------------------------
2020-04-01 21:37:45,620 EPOCH 5 done: loss 2.3817 - lr 0.0100
2020-04-01 21:37:49,539 DEV : loss 1.4545732736587524 - score 0.8467
2020-04-01 21:37:49,636 BAD EPOCHS (no improvement): 0
2020-04-01 21:37:43,273 epoch 2 - iter 129/439 - loss 3.16415327 - samples/sec: 152.66
2020-04-01 21:37:58,214 epoch 2 - iter 172/439 - loss 3.08355412 - samples/sec: 154.53
------------------------------------
2020-04-01 21:37:56,300 epoch 6 - iter 22/220 - loss 2.16660491 - samples/sec: 264.25
2020-04-01 21:38:04,749 epoch 2 - iter 258/439 - loss 2.97694567 - samples/sec: 149.66
2020-04-01 21:38:06,719 epoch 6 - iter 44/220 - loss 2.19433539 - samples/sec: 299.36
2020-04-01 21:38:16,089 epoch 6 - iter 66/220 - loss 2.16100183 - samples/sec: 335.41

2020-04-01 21:38:12,566 epoch 2 - iter 215/439 - loss 3.00200265 - samples/sec: 156.71
2020-04-01 21:38:27,121 epoch 2 - iter 258/439 - loss 2.94925553 - samples/sec: 155.89
2020-04-01 21:38:33,402 epoch 2 - iter 344/439 - loss 2.92821742 - samples/sec: 149.96
2020-04-01 21:38:36,569 epoch 6 - iter 110/220 - loss 2.21361254 - samples/sec: 322.67
2020-04-01 21:38:46,531 epoch 6 - iter 132/220 - loss 2.20887944 - samples/sec: 325.46
2020-04-01 21:38:41,909 epoch 2 - iter 301/439 - loss 2.93458247 - samples/sec: 151.81
2020-04-01 21:38:57,129 epoch 2 - iter 344/439 - loss 2.86467131 - samples/sec: 154.65
2020-04-01 21:39:01,924 epoch 2 - iter 430/439 - loss 2.86299448 - samples/sec: 153.96
2020-04-01 21:39:09,610 ----------------------------------------------------------------------------------------------------
2020-04-01 21:39:09,610 EPOCH 2 done: loss 2.8572 - lr 0.0100
2020-04-01 21:39:07,095 epoch 6 - iter 176/220 - loss 2.20298877 - samples/sec: 328.91
2020-04-01 21:39:17,217 epoch 6 - iter 198/220 - loss 2.18213246 - samples/sec: 316.14
131 BAD EPOCHS (no improvement): 0
2020-04-01 21:39:15,394 ----------------------------------------------------------------------------------------------------
2020-04-01 21:39:12,273 epoch 2 - iter 387/439 - loss 2.81064785 - samples/sec: 153.96
2020-04-01 21:39:24,345 epoch 3 - iter 43/439 - loss 2.65878336 - samples/sec: 153.82
2020-04-01 21:39:27,617 epoch 2 - iter 430/439 - loss 2.76734592 - samples/sec: 149.21
2020-04-01 21:39:39,512 epoch 3 - iter 86/439 - loss 2.50912381 - samples/sec: 157.71
2020-04-01 21:39:33,646 ----------------------------------------------------------------------------------------------------
2020-04-01 21:39:33,646 EPOCH 6 done: loss 2.1749 - lr 0.0100
2020-04-01 21:39:38,206 DEV : loss 1.3680142164230347 - score 0.8573
2020-04-01 21:39:38,303 BAD EPOCHS (no improvement): 0
2020-04-01 21:39:39,522 ----------------------------------------------------------------------------------------------------
2020-04-01 21:39:35,566 ----------------------------------------------------------------------------------------------------
2020-04-01 21:39:35,567 EPOCH 2 done: loss 2.7620 - lr 0.0100
2020-04-01 21:39:40,034 DEV : loss 1.704492211341858 - score 0.8192
2020-04-01 21:39:40,132 BAD EPOCHS (no improvement): 0
2020-04-01 21:39:41,696 ----------------------------------------------------------------------------------------------------
2020-04-01 21:39:50,445 epoch 3 - iter 43/439 - loss 2.28576596 - samples/sec: 157.36
2020-04-01 21:39:53,492 epoch 7 - iter 44/220 - loss 2.13552532 - samples/sec: 325.13

2020-04-01 21:40:03,862 epoch 7 - iter 66/220 - loss 2.14883314 - samples/sec: 312.97

2020-04-01 21:40:05,330 epoch 3 - iter 86/439 - loss 2.33192456 - samples/sec: 148.83
2020-04-01 21:40:13,374 epoch 7 - iter 88/220 - loss 2.14331494 - samples/sec: 331.61

2020-04-01 21:40:23,394 epoch 7 - iter 110/220 - loss 2.13746674 - samples/sec: 320.05
2020-04-01 21:40:34,065 epoch 7 - iter 132/220 - loss 2.11088457 - samples/sec: 319.64
2020-04-01 21:40:34,515 epoch 3 - iter 172/439 - loss 2.30076522 - samples/sec: 150.51
2020-04-01 21:40:49,061 epoch 3 - iter 215/439 - loss 2.30701990 - samples/sec: 154.65
2020-04-01 21:40:43,836 epoch 7 - iter 154/220 - loss 2.09121851 - samples/sec: 338.23
2020-04-01 21:40:54,709 epoch 7 - iter 176/220 - loss 2.07414247 - samples/sec: 293.30
2020-04-01 21:41:04,763 epoch 7 - iter 198/220 - loss 2.06793714 - samples/sec: 311.28
2020-04-01 21:41:02,756 epoch 3 - iter 258/439 - loss 2.29832838 - samples/sec: 158.73
2020-04-01 21:41:14,867 epoch 7 - iter 220/220 - loss 2.06164550 - samples/sec: 332.37
2020-04-01 21:41:20,878 ----------------------------------------------------------------------------------------------------
2020-04-01 21:41:20,878 EPOCH 7 done: loss 2.0616 - lr 0.0100
2020-04-01 21:41:24,805 DEV : loss 1.275213599205017 - score 0.8673
2020-04-01 21:41:24,904 BAD EPOCHS (no improvement): 0
2020-04-01 21:41:26,248 ----------------------------------------------------------------------------------------------------
2020-04-01 21:41:30,247 epoch 8 - iter 22/220 - loss 2.04286985 - samples/sec: 352.38
2020-04-01 21:41:39,757 epoch 8 - iter 44/220 - loss 1.96595742 - samples/sec: 415.47

2020-04-01 21:41:32,950 epoch 3 - iter 344/439 - loss 2.27300712 - samples/sec: 151.07
2020-04-01 21:41:48,036 epoch 8 - iter 66/220 - loss 2.00640383 - samples/sec: 444.12
-------------------------------------
2020-04-01 21:41:46,384 EPOCH 3 done: loss 2.3083 - lr 0.0100
2020-04-01 21:41:50,826 DEV : loss 1.4465934038162231 - score 0.8499
2020-04-01 21:41:50,923 BAD EPOCHS (no improvement): 0
2020-04-01 21:41:47,244 epoch 3 - iter 387/439 - loss 2.24959243 - samples/sec: 164.49
2020-04-01 21:41:57,295 epoch 8 - iter 88/220 - loss 2.03183837 - samples/sec: 399.05
-------------------------------------
2020-04-01 21:42:00,922 epoch 4 - iter 43/439 - loss 2.09336041 - samples/sec: 157.73
2020-04-01 21:42:02,041 epoch 3 - iter 430/439 - loss 2.22058238 - samples/sec: 159.85
2020-04-01 21:42:07,155 epoch 8 - iter 110/220 - loss 2.00193302 - samples/sec: 368.24
------------------------------------
2020-04-01 21:42:09,789 EPOCH 3 done: loss 2.2192 - lr 0.0100
2020-04-01 21:42:15,521 epoch 4 - iter 86/439 - loss 2.09201870 - samples/sec: 161.40
2020-04-01 21:42:14,251 DEV : loss 1.4527064561843872 - score 0.8459
2020-04-01 21:42:14,350 BAD EPOCHS (no improvement): 0
2020-04-01 21:42:15,897 ----------------------------------------------------------------------------------------------------
2020-04-01 21:42:15,836 epoch 8 - iter 132/220 - loss 1.96915596 - samples/sec: 411.49
2020-04-01 21:42:25,103 epoch 8 - iter 154/220 - loss 1.96012258 - samples/sec: 415.09
2020-04-01 21:42:24,977 epoch 4 - iter 43/439 - loss 2.05819545 - samples/sec: 151.62
2020-04-01 21:42:33,878 epoch 8 - iter 176/220 - loss 1.95264421 - samples/sec: 441.32
2020-04-01 21:42:41,959 epoch 8 - iter 198/220 - loss 1.96255601 - samples/sec: 439.16
2020-04-01 21:42:43,824 epoch 1 - iter 301/439 - loss 5.29225222 - samples/sec: 18.25
2020-04-01 21:42:44,461 epoch 4 - iter 172/439 - loss 2.05093456 - samples/sec: 154.92
2020-04-01 21:42:52,686 epoch 8 - iter 220/220 - loss 1.94824966 - samples/sec: 296.74
2020-04-01 21:42:58,830 ----------------------------------------------------------------------------------------------------
2020-04-01 21:42:58,830 EPOCH 8 done: loss 1.9482 - lr 0.0100
2020-04-01 21:43:02,742 DEV : loss 1.2007293701171875 - score 0.8683
2020-04-01 21:43:02,841 BAD EPOCHS (no improvement): 0
2020-04-01 21:43:04,106 ----------------------------------------------------------------------------------------------------
2020-04-01 21:43:08,540 epoch 9 - iter 22/220 - loss 1.82617320 - samples/sec: 317.86
2020-04-01 21:43:19,211 epoch 9 - iter 44/220 - loss 1.84777444 - samples/sec: 294.07

2020-04-01 21:43:29,542 epoch 9 - iter 66/220 - loss 1.84459770 - samples/sec: 306.91

2020-04-01 21:43:24,661 epoch 4 - iter 215/439 - loss 1.99320234 - samples/sec: 149.32
2020-04-01 21:43:41,095 epoch 9 - iter 88/220 - loss 1.84994867 - samples/sec: 294.75

2020-04-01 21:43:50,824 epoch 9 - iter 110/220 - loss 1.83930867 - samples/sec: 367.33
2020-04-01 21:44:00,748 epoch 9 - iter 132/220 - loss 1.83039615 - samples/sec: 326.59
2020-04-01 21:43:54,650 epoch 4 - iter 301/439 - loss 1.97795234 - samples/sec: 153.62
2020-04-01 21:44:10,427 epoch 9 - iter 154/220 - loss 1.84792253 - samples/sec: 327.04
2020-04-01 21:44:12,445 epoch 4 - iter 430/439 - loss 2.01634902 - samples/sec: 153.19
2020-04-01 21:44:09,197 epoch 4 - iter 344/439 - loss 1.95785314 - samples/sec: 151.84
2020-04-01 21:44:20,434 epoch 9 - iter 176/220 - loss 1.83977492 - samples/sec: 319.90
------------------------------------
2020-04-01 21:44:19,787 EPOCH 4 done: loss 2.0207 - lr 0.0100
2020-04-01 21:44:31,423 epoch 9 - iter 198/220 - loss 1.82202780 - samples/sec: 340.48
4,322 BAD EPOCHS (no improvement): 0
2020-04-01 21:44:25,640 ----------------------------------------------------------------------------------------------------
2020-04-01 21:44:23,270 epoch 4 - iter 387/439 - loss 1.95705066 - samples/sec: 159.46
2020-04-01 21:44:40,926 epoch 9 - iter 220/220 - loss 1.82671459 - samples/sec: 330.63
020-04-01 21:44:38,814 epoch 4 - iter 430/439 - loss 1.93929782 - samples/sec: 155.19
2020-04-01 21:44:47,463 ----------------------------------------------------------------------------------------------------
2020-04-01 21:44:47,463 EPOCH 9 done: loss 1.8267 - lr 0.0100
2020-04-01 21:44:51,385 DEV : loss 1.1279640197753906 - score 0.8774
2020-04-01 21:44:51,482 BAD EPOCHS (no improvement): 0
2020-04-01 21:44:46,853 ----------------------------------------------------------------------------------------------------
2020-04-01 21:44:46,854 EPOCH 4 done: loss 1.9394 - lr 0.0100
2020-04-01 21:44:51,315 DEV : loss 1.2620447874069214 - score 0.8666
2020-04-01 21:44:51,412 BAD EPOCHS (no improvement): 0
2020-04-01 21:44:53,964 ----------------------------------------------------------------------------------------------------
2020-04-01 21:45:02,859 epoch 5 - iter 43/439 - loss 1.74750936 - samples/sec: 154.79

2020-04-01 21:45:09,653 epoch 10 - iter 44/220 - loss 1.81162738 - samples/sec: 272.70
2020-04-01 21:45:19,592 epoch 10 - iter 66/220 - loss 1.79101694 - samples/sec: 328.52
2020-04-01 21:45:17,092 epoch 5 - iter 86/439 - loss 1.78394671 - samples/sec: 151.08
2020-04-01 21:45:32,060 epoch 5 - iter 129/439 - loss 1.85722717 - samples/sec: 156.49
2020-04-01 21:45:40,174 epoch 10 - iter 110/220 - loss 1.77114039 - samples/sec: 312.02
2020-04-01 21:45:50,691 epoch 10 - iter 132/220 - loss 1.76667174 - samples/sec: 314.87
2020-04-01 21:45:46,197 epoch 5 - iter 172/439 - loss 1.83710702 - samples/sec: 156.92
2020-04-01 21:46:00,827 epoch 5 - iter 215/439 - loss 1.80370729 - samples/sec: 151.36

2020-04-01 21:46:12,037 epoch 10 - iter 176/220 - loss 1.76539801 - samples/sec: 290.18
2020-04-01 21:46:21,935 epoch 10 - iter 198/220 - loss 1.76230999 - samples/sec: 283.11
020-04-01 21:46:16,185 epoch 5 - iter 258/439 - loss 1.78259088 - samples/sec: 151.00
2020-04-01 21:46:31,732 epoch 10 - iter 220/220 - loss 1.76746280 - samples/sec: 339.69
2020-04-01 21:46:36,775 ----------------------------------------------------------------------------------------------------
2020-04-01 21:46:36,776 EPOCH 10 done: loss 1.7675 - lr 0.0100
2020-04-01 21:46:40,699 DEV : loss 1.0893893241882324 - score 0.8848
2020-04-01 21:46:40,795 BAD EPOCHS (no improvement): 0
2020-04-01 21:46:42,135 ----------------------------------------------------------------------------------------------------
2020-04-01 21:46:46,339 epoch 11 - iter 22/220 - loss 1.63370965 - samples/sec: 335.26
2020-04-01 21:46:45,354 epoch 5 - iter 344/439 - loss 1.77337415 - samples/sec: 151.80
2020-04-01 21:46:56,079 epoch 11 - iter 44/220 - loss 1.74600538 - samples/sec: 332.03
------------------------------------
2020-04-01 21:46:59,288 EPOCH 5 done: loss 1.8676 - lr 0.0100
2020-04-01 21:47:03,679 DEV : loss 1.1658430099487305 - score 0.8743
2020-04-01 21:47:03,773 BAD EPOCHS (no improvement): 0
2020-04-01 21:47:00,069 epoch 5 - iter 387/439 - loss 1.77302968 - samples/sec: 152.27
2020-04-01 21:47:06,395 epoch 11 - iter 66/220 - loss 1.73971033 - samples/sec: 336.73
------------------------------------
2020-04-01 21:47:14,301 epoch 6 - iter 43/439 - loss 1.81863330 - samples/sec: 149.63
2020-04-01 21:47:16,079 epoch 5 - iter 430/439 - loss 1.75848615 - samples/sec: 153.49
2020-04-01 21:47:23,941 ----------------------------------------------------------------------------------------------------
2020-04-01 21:47:23,942 EPOCH 5 done: loss 1.7539 - lr 0.0100
2020-04-01 21:47:27,257 epoch 11 - iter 110/220 - loss 1.72509954 - samples/sec: 343.062020-04-01 21:47:28,420 DEV : loss 1.1610677242279053 - score 0.8752
2020-04-01 21:47:28,520 BAD EPOCHS (no improvement): 0
2020-04-01 21:47:30,157 ----------------------------------------------------------------------------------------------------
2020-04-01 21:47:44,343 epoch 6 - iter 129/439 - loss 1.79080721 - samples/sec: 157.26
2020-04-01 21:47:39,309 epoch 6 - iter 43/439 - loss 1.68767588 - samples/sec: 150.44

2020-04-01 21:47:53,834 epoch 6 - iter 86/439 - loss 1.65148848 - samples/sec: 152.88

2020-04-01 21:47:54,886 epoch 11 - iter 176/220 - loss 1.70115935 - samples/sec: 452.10
2020-04-01 21:48:03,807 epoch 11 - iter 198/220 - loss 1.70054864 - samples/sec: 415.86
2020-04-01 21:48:13,302 epoch 11 - iter 220/220 - loss 1.70434876 - samples/sec: 451.552020-04-01 21:48:08,508 epoch 6 - iter 129/439 - loss 1.66262347 - samples/sec: 150.00
2020-04-01 21:48:23,272 epoch 6 - iter 172/439 - loss 1.68654476 - samples/sec: 153.70
------------------------------------
2020-04-01 21:48:18,556 EPOCH 11 done: loss 1.7043 - lr 0.0100
2020-04-01 21:48:22,327 DEV : loss 1.0230379104614258 - score 0.889
2020-04-01 21:48:22,426 BAD EPOCHS (no improvement): 0
2020-04-01 21:48:23,613 ----------------------------------------------------------------------------------------------------
2020-04-01 21:48:26,709 epoch 12 - iter 22/220 - loss 1.80518748 - samples/sec: 455.13
2020-04-01 21:48:35,459 epoch 12 - iter 44/220 - loss 1.77231604 - samples/sec: 456.52
2020-04-01 21:48:38,158 epoch 6 - iter 215/439 - loss 1.64102029 - samples/sec: 154.08
2020-04-01 21:48:48,069 epoch 1 - iter 344/439 - loss 4.96443959 - samples/sec: 18.18
2020-04-01 21:48:52,626 epoch 6 - iter 258/439 - loss 1.65250879 - samples/sec: 150.50
2020-04-01 21:48:57,287 epoch 6 - iter 344/439 - loss 1.71208084 - samples/sec: 153.83
2020-04-01 21:49:01,567 epoch 12 - iter 110/220 - loss 1.67465328 - samples/sec: 444.64
2020-04-01 21:49:11,307 epoch 12 - iter 132/220 - loss 1.64432798 - samples/sec: 346.072020-04-01 21:49:07,115 epoch 6 - iter 301/439 - loss 1.64996905 - samples/sec: 152.49
2020-04-01 21:49:21,895 epoch 6 - iter 344/439 - loss 1.62419509 - samples/sec: 152.20

2020-04-01 21:49:27,568 epoch 6 - iter 430/439 - loss 1.70643118 - samples/sec: 153.46
2020-04-01 21:49:31,113 epoch 12 - iter 176/220 - loss 1.63095013 - samples/sec: 316.45
2020-04-01 21:49:41,523 epoch 12 - iter 198/220 - loss 1.63245380 - samples/sec: 312.90
-----------------------------------
2020-04-01 21:49:35,057 EPOCH 6 done: loss 1.7084 - lr 0.0100
2020-04-01 21:49:39,502 DEV : loss 1.0917867422103882 - score 0.8792
2020-04-01 21:49:39,598 BAD EPOCHS (no improvement): 0
2020-04-01 21:49:40,989 ----------------------------------------------------------------------------------------------------
2020-04-01 21:49:35,595 epoch 6 - iter 387/439 - loss 1.62275309 - samples/sec: 155.18
2020-04-01 21:49:50,010 epoch 7 - iter 43/439 - loss 1.58709106 - samples/sec: 152.63
2020-04-01 21:49:51,487 epoch 6 - iter 430/439 - loss 1.60658270 - samples/sec: 152.40

2020-04-01 21:50:04,875 epoch 7 - iter 86/439 - loss 1.54316079 - samples/sec: 164.82
2020-04-01 21:49:57,916 ----------------------------------------------------------------------------------------------------
2020-04-01 21:49:57,916 EPOCH 12 done: loss 1.6422 - lr 0.0100
2020-04-01 21:50:01,859 DEV : loss 1.0198478698730469 - score 0.8905
2020-04-01 21:50:01,956 BAD EPOCHS (no improvement): 0
2020-04-01 21:50:03,161 ----------------------------------------------------------------------------------------------------
2020-04-01 21:49:59,542 ----------------------------------------------------------------------------------------------------
2020-04-01 21:49:59,543 EPOCH 6 done: loss 1.6076 - lr 0.0100
2020-04-01 21:50:03,974 DEV : loss 1.0791224241256714 - score 0.883
2020-04-01 21:50:04,069 BAD EPOCHS (no improvement): 0
2020-04-01 21:50:05,506 ----------------------------------------------------------------------------------------------------
2020-04-01 21:50:14,542 epoch 7 - iter 43/439 - loss 1.55998673 - samples/sec: 152.37
2020-04-01 21:50:18,323 epoch 13 - iter 44/220 - loss 1.64513641 - samples/sec: 314.49
2020-04-01 21:50:28,148 epoch 13 - iter 66/220 - loss 1.63187860 - samples/sec: 320.02
2020-04-01 21:50:28,526 epoch 7 - iter 86/439 - loss 1.61170508 - samples/sec: 154.59
2020-04-01 21:50:43,402 epoch 7 - iter 129/439 - loss 1.58554847 - samples/sec: 151.49
2020-04-01 21:50:39,027 epoch 13 - iter 88/220 - loss 1.61736178 - samples/sec: 277.10
2020-04-01 21:50:50,509 epoch 7 - iter 215/439 - loss 1.55431106 - samples/sec: 155.47
2020-04-01 21:50:49,449 epoch 13 - iter 110/220 - loss 1.62177719 - samples/sec: 306.11
2020-04-01 21:51:00,407 epoch 13 - iter 132/220 - loss 1.61235202 - samples/sec: 284.48
020-04-01 21:51:05,166 epoch 7 - iter 258/439 - loss 1.58111041 - samples/sec: 157.47
2020-04-01 21:50:57,328 epoch 7 - iter 172/439 - loss 1.56078806 - samples/sec: 154.99
2020-04-01 21:51:11,461 epoch 13 - iter 154/220 - loss 1.58739590 - samples/sec: 279.66
2020-04-01 21:51:21,704 epoch 13 - iter 176/220 - loss 1.57277068 - samples/sec: 293.84
2020-04-01 21:51:32,015 epoch 13 - iter 198/220 - loss 1.56818258 - samples/sec: 293.75
020-04-01 21:51:26,641 epoch 7 - iter 258/439 - loss 1.58239161 - samples/sec: 151.90
2020-04-01 21:51:42,789 epoch 13 - iter 220/220 - loss 1.56477538 - samples/sec: 294.12
2020-04-01 21:51:49,140 epoch 7 - iter 387/439 - loss 1.60558063 - samples/sec: 154.25
2020-04-01 21:51:49,302 ----------------------------------------------------------------------------------------------------
2020-04-01 21:51:49,302 EPOCH 13 done: loss 1.5648 - lr 0.0100
2020-04-01 21:51:53,229 DEV : loss 0.9595125317573547 - score 0.8949
2020-04-01 21:51:53,325 BAD EPOCHS (no improvement): 0
2020-04-01 21:51:54,643 ----------------------------------------------------------------------------------------------------
2020-04-01 21:51:58,790 epoch 14 - iter 22/220 - loss 1.51173158 - samples/sec: 339.81
2020-04-01 21:51:56,103 epoch 7 - iter 344/439 - loss 1.56065140 - samples/sec: 153.44
2020-04-01 21:52:12,928 ----------------------------------------------------------------------------------------------------
2020-04-01 21:52:12,928 EPOCH 7 done: loss 1.6083 - lr 0.0100
2020-04-01 21:52:10,605 epoch 7 - iter 387/439 - loss 1.55124961 - samples/sec: 170.68
2020-04-01 21:52:10,008 epoch 14 - iter 44/220 - loss 1.51242736 - samples/sec: 292.93
2020-04-01 21:52:21,089 epoch 14 - iter 66/220 - loss 1.53968004 - samples/sec: 313.78
7,387 BAD EPOCHS (no improvement): 0
2020-04-01 21:52:18,740 ----------------------------------------------------------------------------------------------------
2020-04-01 21:52:32,659 epoch 14 - iter 88/220 - loss 1.53801650 - samples/sec: 302.20
020-04-01 21:52:26,301 epoch 7 - iter 430/439 - loss 1.54199089 - samples/sec: 154.06
2020-04-01 21:52:34,969 ----------------------------------------------------------------------------------------------------
2020-04-01 21:52:34,969 EPOCH 7 done: loss 1.5428 - lr 0.0100
2020-04-01 21:52:43,208 epoch 14 - iter 110/220 - loss 1.54125435 - samples/sec: 326.14
20-04-01 21:52:39,453 DEV : loss 1.0382102727890015 - score 0.8874
2020-04-01 21:52:39,551 BAD EPOCHS (no improvement): 0
2020-04-01 21:52:41,109 ----------------------------------------------------------------------------------------------------
2020-04-01 21:52:53,709 epoch 14 - iter 132/220 - loss 1.53412529 - samples/sec: 328.40
020-04-01 21:52:50,054 epoch 8 - iter 43/439 - loss 1.54923632 - samples/sec: 153.90
2020-04-01 21:53:03,599 epoch 14 - iter 154/220 - loss 1.54635927 - samples/sec: 323.77
2020-04-01 21:53:14,009 epoch 14 - iter 176/220 - loss 1.53796637 - samples/sec: 341.68
2020-04-01 21:53:23,068 epoch 14 - iter 198/220 - loss 1.53540944 - samples/sec: 423.91
020-04-01 21:53:20,830 epoch 8 - iter 129/439 - loss 1.50661691 - samples/sec: 152.54
2020-04-01 21:53:32,876 epoch 14 - iter 220/220 - loss 1.53395760 - samples/sec: 424.83
2020-04-01 21:53:38,599 ----------------------------------------------------------------------------------------------------
2020-04-01 21:53:38,599 EPOCH 14 done: loss 1.5340 - lr 0.0100
2020-04-01 21:53:43,168 DEV : loss 0.9630600214004517 - score 0.8976
2020-04-01 21:53:43,266 BAD EPOCHS (no improvement): 0
2020-04-01 21:53:44,469 ----------------------------------------------------------------------------------------------------
2020-04-01 21:53:47,959 epoch 15 - iter 22/220 - loss 1.45197306 - samples/sec: 403.68
2020-04-01 21:53:50,985 epoch 8 - iter 215/439 - loss 1.47931425 - samples/sec: 151.25
2020-04-01 21:53:57,129 epoch 15 - iter 44/220 - loss 1.45900394 - samples/sec: 430.88
2020-04-01 21:54:06,361 epoch 15 - iter 66/220 - loss 1.47710453 - samples/sec: 402.33
2020-04-01 21:54:15,666 epoch 15 - iter 88/220 - loss 1.44124281 - samples/sec: 455.95
2020-04-01 21:54:24,479 epoch 15 - iter 110/220 - loss 1.43323888 - samples/sec: 471.92
020-04-01 21:54:21,222 epoch 8 - iter 301/439 - loss 1.47405698 - samples/sec: 150.72
2020-04-01 21:54:24,529 Reading data from data
2020-04-01 21:54:24,529 Train: data/train.txt
2020-04-01 21:54:24,529 Dev: data/valid.txt
2020-04-01 21:54:24,529 Test: data/test.txt
Corpus: 14041 train + 3250 dev + 3453 test sentences
Dictionary with 12 tags: <unk>, O, B-ORG, B-MISC, B-PER, I-PER, B-LOC, I-ORG, I-MISC, I-LOC, <START>, <STOP>
bert
2020-04-01 21:54:35,837 epoch 8 - iter 344/439 - loss 1.47392947 - samples/sec: 160.12
2020-04-01 21:54:34,636 epoch 15 - iter 132/220 - loss 1.42712537 - samples/sec: 451.15
2020-04-01 21:54:33,724 epoch 2 - iter 198/220 - loss 2.48358749 - samples/sec: 127.43
2020-04-01 21:54:39,565 epoch 8 - iter 430/439 - loss 1.50801164 - samples/sec: 152.46
2020-04-01 21:54:50,414 Reading data from data
2020-04-01 21:54:50,414 Train: data/train.txt
2020-04-01 21:54:50,414 Dev: data/valid.txt
2020-04-01 21:54:50,414 Test: data/test.txt
2020-04-01 21:54:46,028 ----------------------------------------------------------------------------------------------------
2020-04-01 21:54:46,031 Model: "SequenceTagger(
  (embeddings): BertEmbeddings(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=3072, out_features=3072, bias=True)
  (rnn): LSTM(3072, 512, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=1024, out_features=12, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2020-04-01 21:54:46,032 ----------------------------------------------------------------------------------------------------
2020-04-01 21:54:46,032 Corpus: "Corpus: 14041 train + 3250 dev + 3453 test sentences"
2020-04-01 21:54:46,032 ----------------------------------------------------------------------------------------------------
2020-04-01 21:54:46,032 Parameters:
2020-04-01 21:54:46,033  - learning_rate: "0.01"
2020-04-01 21:54:46,033  - mini_batch_size: "32"
2020-04-01 21:54:46,033  - patience: "3"
2020-04-01 21:54:46,033  - anneal_factor: "0.5"
2020-04-01 21:54:46,033  - max_epochs: "150"
2020-04-01 21:54:46,033  - shuffle: "True"
2020-04-01 21:54:46,033  - train_with_dev: "False"
2020-04-01 21:54:46,033  - batch_growth_annealing: "False"
2020-04-01 21:54:46,033 ----------------------------------------------------------------------------------------------------
2020-04-01 21:54:46,033 Model training base path: "log/bert_20200401215445_512"
2020-04-01 21:54:46,034 ----------------------------------------------------------------------------------------------------
2020-04-01 21:54:46,034 Device: cuda:0
2020-04-01 21:54:46,034 ----------------------------------------------------------------------------------------------------
2020-04-01 21:54:46,034 Embeddings storage mode: cpu
2020-04-01 21:54:46,042 ----------------------------------------------------------------------------------------------------
2020-04-01 21:54:51,923 ----------------------------------------------------------------------------------------------------
2020-04-01 21:54:51,923 EPOCH 8 done: loss 1.5066 - lr 0.0100
2020-04-01 21:54:56,366 DEV : loss 0.9768828749656677 - score 0.8907
2020-04-01 21:55:00,937 epoch 8 - iter 387/439 - loss 1.46021805 - samples/sec: 131.45

2020-04-01 21:54:57,457 epoch 1 - iter 387/439 - loss 4.68698699 - samples/sec: 18.17
Corpus: 14041 train + 3250 dev + 3453 test sentences
Dictionary with 12 tags: <unk>, O, B-ORG, B-MISC, B-PER, I-PER, B-LOC, I-ORG, I-MISC, I-LOC, <START>, <STOP>
bert
2020-04-01 21:54:59,257 epoch 1 - iter 43/439 - loss 18.23910446 - samples/sec: 104.17
2020-04-01 21:54:56,463 BAD EPOCHS (no improvement): 0
2020-04-01 21:55:03,680 ----------------------------------------------------------------------------------------------------
2020-04-01 21:55:07,036 ----------------------------------------------------------------------------------------------------
2020-04-01 21:55:07,038 Model: "SequenceTagger(
  (embeddings): BertEmbeddings(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=3072, out_features=3072, bias=True)
  (rnn): LSTM(3072, 128, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=256, out_features=12, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2020-04-01 21:55:07,038 ----------------------------------------------------------------------------------------------------
2020-04-01 21:55:07,038 Corpus: "Corpus: 14041 train + 3250 dev + 3453 test sentences"
2020-04-01 21:55:07,038 ----------------------------------------------------------------------------------------------------
2020-04-01 21:55:07,039 Parameters:
2020-04-01 21:55:07,039  - learning_rate: "0.01"
2020-04-01 21:55:07,039  - mini_batch_size: "32"
2020-04-01 21:55:07,039  - patience: "3"
2020-04-01 21:55:07,039  - anneal_factor: "0.5"
2020-04-01 21:55:07,039  - max_epochs: "150"
2020-04-01 21:55:07,039  - shuffle: "True"
2020-04-01 21:55:07,039  - train_with_dev: "False"
2020-04-01 21:55:07,039  - batch_growth_annealing: "False"
2020-04-01 21:55:07,039 ----------------------------------------------------------------------------------------------------
2020-04-01 21:55:07,039 Model training base path: "log/bert_20200401215506_128"
2020-04-01 21:55:07,039 ----------------------------------------------------------------------------------------------------
2020-04-01 21:55:07,039 Device: cuda:0
2020-04-01 21:55:07,039 ----------------------------------------------------------------------------------------------------
2020-04-01 21:55:07,040 Embeddings storage mode: cpu
2020-04-01 21:55:07,043 ----------------------------------------------------------------------------------------------------
2020-04-01 21:55:12,680 epoch 9 - iter 43/439 - loss 1.44061262 - samples/sec: 152.97
2020-04-01 21:55:13,817 epoch 15 - iter 176/220 - loss 1.46727217 - samples/sec: 301.37
2020-04-01 21:55:18,753 epoch 8 - iter 430/439 - loss 1.45238254 - samples/sec: 167.08
2020-04-01 21:55:21,550 epoch 1 - iter 43/439 - loss 18.83657748 - samples/sec: 94.88
2020-04-01 21:55:24,342 epoch 9 - iter 86/439 - loss 1.46337468 - samples/sec: 273.24

2020-04-01 21:55:28,295 ----------------------------------------------------------------------------------------------------
2020-04-01 21:55:28,295 EPOCH 8 done: loss 1.4554 - lr 0.0100
2020-04-01 21:55:32,748 DEV : loss 0.9808292984962463 - score 0.8952
2020-04-01 21:55:32,846 BAD EPOCHS (no improvement): 0
2020-04-01 21:55:37,021 epoch 15 - iter 220/220 - loss 1.48314253 - samples/sec: 319.53
2020-04-01 21:55:34,364 ----------------------------------------------------------------------------------------------------
2020-04-01 21:55:42,223 epoch 9 - iter 43/439 - loss 1.47077640 - samples/sec: 175.19
2020-04-01 21:55:48,247 epoch 9 - iter 172/439 - loss 1.46705744 - samples/sec: 307.80
------------------------------------
2020-04-01 21:55:44,310 EPOCH 15 done: loss 1.4831 - lr 0.0100
2020-04-01 21:55:48,359 DEV : loss 0.9177575707435608 - score 0.9002
2020-04-01 21:55:48,456 BAD EPOCHS (no improvement): 0
2020-04-01 21:55:50,016 ----------------------------------------------------------------------------------------------------
2020-04-01 21:55:59,330 epoch 9 - iter 215/439 - loss 1.47314890 - samples/sec: 337.79
2020-04-01 21:55:57,789 epoch 9 - iter 86/439 - loss 1.45282372 - samples/sec: 162.72
2020-04-01 21:56:11,136 epoch 9 - iter 258/439 - loss 1.45996768 - samples/sec: 305.75
2020-04-01 21:56:18,560 epoch 16 - iter 66/220 - loss 1.44822650 - samples/sec: 294.58
2020-04-01 21:56:13,220 epoch 9 - iter 129/439 - loss 1.44839018 - samples/sec: 165.23
2020-04-01 21:56:31,180 epoch 16 - iter 88/220 - loss 1.44960806 - samples/sec: 306.54
2020-04-01 21:56:31,426 epoch 9 - iter 172/439 - loss 1.43673607 - samples/sec: 156.99
2020-04-01 21:56:43,059 epoch 16 - iter 110/220 - loss 1.45639673 - samples/sec: 348.16
2020-04-01 21:56:44,927 epoch 9 - iter 344/439 - loss 1.44758897 - samples/sec: 157.48
2020-04-01 21:56:47,890 epoch 9 - iter 215/439 - loss 1.40981872 - samples/sec: 161.89
2020-04-01 21:57:01,098 epoch 9 - iter 387/439 - loss 1.43368162 - samples/sec: 151.59
2020-04-01 21:56:53,899 epoch 16 - iter 132/220 - loss 1.46787085 - samples/sec: 366.92
2020-04-01 21:57:04,186 epoch 16 - iter 154/220 - loss 1.46245877 - samples/sec: 334.58
2020-04-01 21:57:15,301 epoch 9 - iter 430/439 - loss 1.42501250 - samples/sec: 167.09
2020-04-01 21:57:15,435 epoch 16 - iter 176/220 - loss 1.45815436 - samples/sec: 297.20
2020-04-01 21:57:16,713 epoch 1 - iter 86/439 - loss 13.50084000 - samples/sec: 86.26
2020-04-01 21:57:24,496 ----------------------------------------------------------------------------------------------------
2020-04-01 21:57:24,497 EPOCH 9 done: loss 1.4213 - lr 0.0100
2020-04-01 21:57:32,722 DEV : loss 0.9550383687019348 - score 0.8983
2020-04-01 21:57:32,820 BAD EPOCHS (no improvement): 0
2020-04-01 21:57:31,916 epoch 1 - iter 86/439 - loss 14.36439381 - samples/sec: 92.67

2020-04-01 21:57:30,003 epoch 9 - iter 344/439 - loss 1.39858716 - samples/sec: 305.18
2020-04-01 21:57:34,332 ----------------------------------------------------------------------------------------------------
2020-04-01 21:57:37,854 epoch 16 - iter 220/220 - loss 1.43910055 - samples/sec: 314.92
2020-04-01 21:57:43,954 ----------------------------------------------------------------2020-04-01 21:57:46,598 epoch 9 - iter 387/439 - loss 1.41239705 - samples/sec: 150.89
 - lr 0.0100
2020-04-01 21:57:47,899 DEV : loss 0.8816548585891724 - score 0.9005
2020-04-01 21:57:47,997 BAD EPOCHS (no improvement): 0
2020-04-01 21:57:49,605 ----------------------------------------------------------------------------------------------------
2020-04-01 21:57:54,609 epoch 17 - iter 22/220 - loss 1.55306437 - samples/sec: 281.58
^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A2020-04-01 21:58:03,463 epoch 9 - iter 430/439 - loss 1.39516708 - samples/sec: 152.81
2020-04-01 21:58:13,685 ----------------------------------------------------------------------------------------------------
2020-04-01 21:58:13,686 EPOCH 9 done: loss 1.3967 - lr 0.0100
2020-04-01 21:58:18,916 epoch 17 - iter 66/220 - loss 1.41068662 - samples/sec: 310.74

2020-04-01 21:58:18,789 DEV : loss 0.9608155488967896 - score 0.8982
2020-04-01 21:58:18,888 BAD EPOCHS (no improvement): 0
2020-04-01 21:58:20,546 ----------------------------------------------------------------------------------------------------
2020-04-01 21:58:29,635 epoch 10 - iter 43/439 - loss 1.41727304 - samples/sec: 151.47
2020-04-01 21:58:35,981 epoch 10 - iter 172/439 - loss 1.37872863 - samples/sec: 154.29
2020-04-01 21:58:44,514 epoch 17 - iter 110/220 - loss 1.40434168 - samples/sec: 268.00
2020-04-01 21:58:45,317 epoch 10 - iter 86/439 - loss 1.32659741 - samples/sec: 155.54
2020-04-01 21:58:53,962 Reading data from data
2020-04-01 21:58:53,962 Train: data/train.txt
2020-04-01 21:58:53,962 Dev: data/valid.txt
2020-04-01 21:58:53,962 Test: data/test.txt
Corpus: 14041 train + 3250 dev + 3453 test sentences
Dictionary with 12 tags: <unk>, O, B-ORG, B-MISC, B-PER, I-PER, B-LOC, I-ORG, I-MISC, I-LOC, <START>, <STOP>
bert
2020-04-01 21:59:02,193 epoch 10 - iter 129/439 - loss 1.30631590 - samples/sec: 163.55
020-04-01 21:58:57,081 epoch 17 - iter 132/220 - loss 1.40848124 - samples/sec: 344.20
2020-04-01 21:59:07,586 epoch 17 - iter 154/220 - loss 1.40470626 - samples/sec: 312.79
2020-04-01 21:59:18,085 epoch 17 - iter 176/220 - loss 1.41401305 - samples/sec: 330.50
2020-04-01 21:59:18,160 epoch 10 - iter 172/439 - loss 1.32892481 - samples/sec: 144.50
Traceback (most recent call last):
  File "train.py", line 132, in <module>
    max_epochs=150)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/trainers/trainer.py", line 331, in train
    loss = self.model.forward_loss(batch_step)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py", line 493, in forward_loss
    features = self.forward(data_points)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py", line 498, in forward
    self.embeddings.embed(sentences)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/embeddings.py", line 96, in embed
    self._add_embeddings_internal(sentences)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/embeddings.py", line 2338, in _add_embeddings_internal
    all_encoder_layers = self.model(all_input_ids, attention_mask=all_input_masks)[
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/transformers/modeling_bert.py", line 790, in forward
    encoder_attention_mask=encoder_extended_attention_mask,
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/transformers/modeling_bert.py", line 407, in forward
    hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/transformers/modeling_bert.py", line 368, in forward
    self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/transformers/modeling_bert.py", line 314, in forward
    hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/transformers/modeling_bert.py", line 234, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 10.73 GiB total capacity; 3.89 GiB already allocated; 36.56 MiB free; 4.46 GiB reserved in total by PyTorch)
2020-04-01 21:59:36,381 epoch 10 - iter 215/439 - loss 1.33406806 - samples/sec: 170.35
2020-04-01 21:59:32,546 ----------------------------------------------------------------------------------------------------
2020-04-01 21:59:32,549 Model: "SequenceTagger(
  (embeddings): BertEmbeddings(
    (model): BertModel(
      (embeddings): BertEmbeddings(
        (word_embeddings): Embedding(30522, 768, padding_idx=0)
        (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=3072, out_features=3072, bias=True)
  (rnn): LSTM(3072, 64, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=128, out_features=12, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2020-04-01 21:59:32,549 ----------------------------------------------------------------------------------------------------
2020-04-01 21:59:32,549 Corpus: "Corpus: 14041 train + 3250 dev + 3453 test sentences"
2020-04-01 21:59:32,549 ----------------------------------------------------------------------------------------------------
2020-04-01 21:59:32,549 Parameters:
2020-04-01 21:59:32,550  - learning_rate: "0.01"
2020-04-01 21:59:32,550  - mini_batch_size: "32"
2020-04-01 21:59:32,550  - patience: "3"
2020-04-01 21:59:32,550  - anneal_factor: "0.5"
2020-04-01 21:59:32,550  - max_epochs: "150"
2020-04-01 21:59:32,550  - shuffle: "True"
2020-04-01 21:59:32,550  - train_with_dev: "False"
2020-04-01 21:59:32,550  - batch_growth_annealing: "False"
2020-04-01 21:59:32,550 ----------------------------------------------------------------------------------------------------
2020-04-01 21:59:32,550 Model training base path: "log/bert_20200401215932_64"
2020-04-01 21:59:32,550 ----------------------------------------------------------------------------------------------------
2020-04-01 21:59:32,550 Device: cuda:0
2020-04-01 21:59:32,550 ----------------------------------------------------------------------------------------------------
2020-04-01 21:59:32,551 Embeddings storage mode: cpu
2020-04-01 21:59:32,554 ----------------------------------------------------------------------------------------------------
2020-04-01 21:59:39,951 epoch 10 - iter 344/439 - loss 1.35328612 - samples/sec: 145.02
2020-04-01 21:59:42,347 epoch 17 - iter 220/220 - loss 1.41120465 - samples/sec: 324.07
2020-04-01 21:59:45,454 epoch 1 - iter 43/439 - loss 14.81685906 - samples/sec: 106.71
2020-04-01 21:59:47,639 epoch 1 - iter 129/439 - loss 12.09737765 - samples/sec: 84.15
2020-04-01 21:59:51,119 epoch 10 - iter 258/439 - loss 1.32563704 - samples/sec: 164.40
-----------------------------------
2020-04-01 21:59:48,844 EPOCH 17 done: loss 1.4112 - lr 0.0100
2020-04-01 21:59:52,770 DEV : loss 0.8678370118141174 - score 0.9061
2020-04-01 21:59:52,867 BAD EPOCHS (no improvement): 0
2020-04-01 21:59:54,169 ----------------------------------------------------------------------------------------------------
2020-04-01 21:59:55,550 epoch 10 - iter 387/439 - loss 1.35914658 - samples/sec: 152.10
2020-04-01 21:59:58,942 epoch 18 - iter 22/220 - loss 1.45376080 - samples/sec: 295.29
2020-04-01 22:00:06,433 epoch 10 - iter 301/439 - loss 1.30420611 - samples/sec: 154.24
2020-04-01 22:00:11,155 epoch 10 - iter 430/439 - loss 1.36304460 - samples/sec: 150.80
2020-04-01 22:00:10,653 epoch 18 - iter 44/220 - loss 1.44350730 - samples/sec: 285.88
2020-04-01 22:00:22,152 epoch 18 - iter 66/220 - loss 1.41003594 - samples/sec: 295.31
------------------------------------
2020-04-01 22:00:19,768 EPOCH 10 done: loss 1.3630 - lr 0.0100
2020-04-01 22:00:21,178 epoch 10 - iter 344/439 - loss 1.31115504 - samples/sec: 156.50
2020-04-01 22:00:24,928 DEV : loss 0.9043971300125122 - score 0.9004
2020-04-01 22:00:25,022 BAD EPOCHS (no improvement): 0
2020-04-01 22:00:26,328 ----------------------------------------------------------------------------------------------------
2020-04-01 22:00:33,466 epoch 18 - iter 88/220 - loss 1.40026486 - samples/sec: 343.85
2020-04-01 22:00:45,383 epoch 18 - iter 110/220 - loss 1.40697594 - samples/sec: 330.47
2020-04-01 22:00:37,925 epoch 10 - iter 387/439 - loss 1.31668852 - samples/sec: 151.11
^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B^[[B2020-04-01 22:00:52,381 epoch 11 - iter 86/439 - loss 1.34986869 - samples/sec: 152.08
2020-04-01 22:00:55,135 epoch 18 - iter 132/220 - loss 1.40172650 - samples/sec: 418.11
2020-04-01 22:00:54,295 epoch 10 - iter 430/439 - loss 1.32239571 - samples/sec: 157.44
2020-04-01 22:01:02,053 ----------------------------------------------------------------------------------------------------
2020-04-01 22:01:02,054 EPOCH 10 done: loss 1.3243 - lr 0.0100
2020-04-01 22:01:06,467 DEV : loss 0.9548019170761108 - score 0.8957
2020-04-01 22:01:06,564 BAD EPOCHS (no improvement): 1
2020-04-01 22:01:06,591 ----------------------------------------------------------------------------------------------------
2020-04-01 22:01:06,470 epoch 11 - iter 129/439 - loss 1.34918560 - samples/sec: 166.66
2020-04-01 22:01:14,165 epoch 18 - iter 176/220 - loss 1.37682163 - samples/sec: 392.89
2020-04-01 22:01:15,334 epoch 11 - iter 43/439 - loss 1.20262373 - samples/sec: 157.47
2020-04-01 22:01:21,210 epoch 11 - iter 172/439 - loss 1.36571110 - samples/sec: 152.90
2020-04-01 22:01:23,085 epoch 18 - iter 198/220 - loss 1.37373448 - samples/sec: 455.11
2020-04-01 22:01:30,162 epoch 11 - iter 86/439 - loss 1.29032152 - samples/sec: 152.08

2020-04-01 22:01:37,895 -----------------------------------------------------------------2020-04-01 22:01:44,809 epoch 11 - iter 129/439 - loss 1.30730593 - samples/sec: 159.23
 lr 0.0100
2020-04-01 22:01:41,709 DEV : loss 0.8458755612373352 - score 0.9092
2020-04-01 22:01:41,809 BAD EPOCHS (no improvement): 0
2020-04-01 22:01:43,012 ----------------------------------------------------------------------------------------------------
2020-04-01 22:01:46,062 epoch 19 - iter 22/220 - loss 1.41248542 - samples/sec: 461.95
2020-04-01 22:01:42,758 epoch 1 - iter 86/439 - loss 11.46792178 - samples/sec: 100.76
2020-04-01 22:01:53,221 epoch 11 - iter 258/439 - loss 1.35376990 - samples/sec: 140.12
2020-04-01 22:01:47,435 epoch 1 - iter 172/439 - loss 10.61539973 - samples/sec: 85.81
2020-04-01 22:01:52,554 Reading data from data
2020-04-01 22:01:52,554 Train: data/train.txt
2020-04-01 22:01:52,554 Dev: data/valid.txt
2020-04-01 22:01:52,554 Test: data/test.txt
Corpus: 14041 train + 3250 dev + 3453 test sentences
Dictionary with 12 tags: <unk>, O, B-ORG, B-MISC, B-PER, I-PER, B-LOC, I-ORG, I-MISC, I-LOC, <START>, <STOP>
2020-04-01 22:02:01,086 epoch 1 - iter 430/439 - loss 4.47602986 - samples/sec: 18.10
2020-04-01 22:01:59,057 epoch 11 - iter 172/439 - loss 1.27550333 - samples/sec: 157.41
2020-04-01 22:02:05,102 epoch 19 - iter 66/220 - loss 1.38562889 - samples/sec: 369.02
2020-04-01 22:02:15,949 epoch 19 - iter 88/220 - loss 1.38791063 - samples/sec: 336.59

2020-04-01 22:02:13,748 epoch 11 - iter 215/439 - loss 1.26943312 - samples/sec: 153.12
2020-04-01 22:02:25,780 epoch 19 - iter 110/220 - loss 1.36841407 - samples/sec: 364.42
2020-04-01 22:02:36,982 epoch 19 - iter 132/220 - loss 1.36382490 - samples/sec: 339.91
2020-04-01 22:02:33,222 ----------------------------------------------------------------------------------------------------
2020-04-01 22:02:33,223 EPOCH 2 done: loss 2.4282 - lr 0.0100
2020-04-01 22:02:39,538 epoch 11 - iter 387/439 - loss 1.33352792 - samples/sec: 152.59
2020-04-01 22:02:47,335 epoch 19 - iter 154/220 - loss 1.35792208 - samples/sec: 334.73
2020-04-01 22:02:47,650 DEV : loss 1.5189762115478516 - score 0.8511
2020-04-01 22:02:48,052 BAD EPOCHS (no improvement): 0
2020-04-01 22:02:58,164 epoch 19 - iter 176/220 - loss 1.34618039 - samples/sec: 306.98
2020-04-01 22:03:02,297 ----------------------------------------------------------------------------------------------------
2020-04-01 22:03:02,298 EPOCH 11 done: loss 1.3169 - lr 0.0100
2020-04-01 22:02:58,560 epoch 11 - iter 344/439 - loss 1.26867280 - samples/sec: 151.05
2020-04-01 22:03:09,081 epoch 19 - iter 198/220 - loss 1.34763021 - samples/sec: 335.29
,837 BAD EPOCHS (no improvement): 0
2020-04-01 22:03:13,855 epoch 11 - iter 387/439 - loss 1.26989604 - samples/sec: 160.96
2020-04-01 22:03:25,635 ----------------------------------------------------------------------------------------------------
2020-04-01 22:03:34,706 epoch 12 - iter 43/439 - loss 1.28271191 - samples/sec: 151.78
2020-04-01 22:03:27,847 ----------------------------------------------------------------------------------------------------
train mode resetting embeddings
train mode resetting embeddings
2020-04-01 22:03:38,330 epoch 19 - iter 220/220 - loss 1.34726993 - samples/sec: 324.59
2020-04-01 22:03:44,853 ----------------------------------------------------------------------------------------------------
2020-04-01 22:03:44,853 EPOCH 19 done: loss 1.3473 - lr 0.0100
2020-04-01 22:03:42,338 epoch 11 - iter 430/439 - loss 1.28490451 - samples/sec: 163.70
2020-04-01 22:03:39,214 epoch 3 - iter 22/220 - loss 2.11951895 - samples/sec: 123.99
2020-04-01 22:03:48,795 DEV : loss 0.8364024758338928 - score 0.91
2020-04-01 22:03:48,894 BAD EPOCHS (no improvement): 0
2020-04-01 22:03:50,197 ----------------------------------------------------------------------------------------------------
2020-04-01 22:03:54,713 epoch 20 - iter 22/220 - loss 1.32854799 - samples/sec: 312.08
      (position_embeddings): Embedding(512, 768)
        (token_type_embeddings): Embedding(2, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (encoder): BertEncoder(
        (layer): ModuleList(
          (0): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (1): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (2): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (3): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (4): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (5): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (6): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (7): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (8): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (9): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (10): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (11): BertLayer(
            (attention): BertAttention(
              (self): BertSelfAttention(
                (query): Linear(in_features=768, out_features=768, bias=True)
                (key): Linear(in_features=768, out_features=768, bias=True)
                (value): Linear(in_features=768, out_features=768, bias=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
              (output): BertSelfOutput(
                (dense): Linear(in_features=768, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (intermediate): BertIntermediate(
              (dense): Linear(in_features=768, out_features=3072, bias=True)
            )
            (output): BertOutput(
              (dense): Linear(in_features=3072, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
        )
      )
      (pooler): BertPooler(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (activation): Tanh()
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=3072, out_features=3072, bias=True)
  (rnn): LSTM(3072, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=12, bias=True)
), SequenceTagger(
  (embeddings): ELMoEmbeddings(model=elmo-small)
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=768, out_features=768, bias=True)
  (rnn): LSTM(768, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=12, bias=True)
)]"
2020-04-01 22:03:48,083 ----------------------------------------------------------------------------------------------------
2020-04-01 22:03:48,083 Corpus: "Corpus: 14041 train + 3250 dev + 3453 test sentences"
2020-04-01 22:03:48,083 ----------------------------------------------------------------------------------------------------
2020-04-01 22:03:48,083 Parameters:
2020-04-01 22:03:48,083  - learning_rate: "0.01"
2020-04-01 22:03:48,083  - mini_batch_size: "32"
2020-04-01 22:03:48,083  - patience: "3"
2020-04-01 22:03:48,083  - anneal_factor: "0.5"
2020-04-01 22:03:48,083  - max_epochs: "150"
2020-04-01 22:03:48,083  - shuffle: "True"
2020-04-01 22:03:48,083  - train_with_dev: "False"
2020-04-01 22:03:48,083  - batch_growth_annealing: "False"
2020-04-01 22:03:48,083 ----------------------------------------------------------------------------------------------------
2020-04-01 22:03:48,083 Model training base path: "log/avg_loss_20200401220348"
2020-04-01 22:03:48,083 ----------------------------------------------------------------------------------------------------
2020-04-01 22:03:48,083 Device: cuda:0
2020-04-01 22:03:48,084 ----------------------------------------------------------------------------------------------------
2020-04-01 22:03:48,084 Embeddings storage mode: cpu
2020-04-01 22:03:48,086 ----------------------------------------------------------------------------------------------------
2020-04-01 22:03:49,890 ----------------------------------------------------------------------------------------------------
2020-04-01 22:03:49,890 EPOCH 11 done: loss 1.2813 - lr 0.0100
2020-04-01 22:03:54,410 DEV : loss 0.8968967199325562 - score 0.9043
2020-04-01 22:03:54,509 BAD EPOCHS (no improvement): 0
2020-04-01 22:04:01,674 epoch 1 - iter 215/439 - loss 9.62748516 - samples/sec: 123.59
2020-04-01 22:03:56,131 ----------------------------------------------------------------------------------------------------
2020-04-01 22:04:04,192 epoch 12 - iter 43/439 - loss 1.16936068 - samples/sec: 170.82
2020-04-01 22:04:11,922 epoch 1 - iter 43/439 - loss 22.17318967 - samples/sec: 57.74
2020-04-01 22:04:06,314 epoch 20 - iter 44/220 - loss 1.33258898 - samples/sec: 307.43
2020-04-01 22:04:07,326 epoch 12 - iter 129/439 - loss 1.26510887 - samples/sec: 149.24
2020-04-01 22:04:18,290 epoch 20 - iter 66/220 - loss 1.29859365 - samples/sec: 299.05

2020-04-01 22:04:19,779 epoch 12 - iter 86/439 - loss 1.20577435 - samples/sec: 154.57
2020-04-01 22:04:35,400 epoch 12 - iter 129/439 - loss 1.26276145 - samples/sec: 164.29
2020-04-01 22:04:40,927 epoch 20 - iter 110/220 - loss 1.29933875 - samples/sec: 307.33
2020-04-01 22:04:52,208 epoch 20 - iter 132/220 - loss 1.31785583 - samples/sec: 373.13
2020-04-01 22:04:50,665 epoch 12 - iter 172/439 - loss 1.25982245 - samples/sec: 165.86
2020-04-01 22:05:03,728 epoch 20 - iter 154/220 - loss 1.32697849 - samples/sec: 443.98
2020-04-01 22:05:14,369 epoch 20 - iter 176/220 - loss 1.32892170 - samples/sec: 428.89
2020-04-01 22:05:07,843 epoch 12 - iter 215/439 - loss 1.26650553 - samples/sec: 150.44
2020-04-01 22:05:24,104 epoch 12 - iter 258/439 - loss 1.24539898 - samples/sec: 149.44
2020-04-01 22:05:35,418 epoch 20 - iter 220/220 - loss 1.32874308 - samples/sec: 425.90
2020-04-01 22:05:42,326 ----------------------------------------------------------------------------------------------------
2020-04-01 22:05:42,326 EPOCH 20 done: loss 1.3287 - lr 0.0100
2020-04-01 22:05:46,233 DEV : loss 0.8259894847869873 - score 0.9113
2020-04-01 22:05:46,331 BAD EPOCHS (no improvement): 0
2020-04-01 22:05:47,990 ----------------------------------------------------------------------------------------------------
2020-04-01 22:05:51,994 epoch 21 - iter 22/220 - loss 1.22412772 - samples/sec: 351.93
2020-04-01 22:06:03,438 epoch 21 - iter 44/220 - loss 1.27588359 - samples/sec: 302.29
2020-04-01 22:06:01,924 epoch 12 - iter 430/439 - loss 1.27054861 - samples/sec: 144.61
Traceback (most recent call last):
  File "train.py", line 132, in <module>
    max_epochs=150)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/trainers/trainer.py", line 331, in train
    loss = self.model.forward_loss(batch_step)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py", line 493, in forward_loss
    features = self.forward(data_points)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py", line 498, in forward
    self.embeddings.embed(sentences)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/embeddings.py", line 96, in embed
    self._add_embeddings_internal(sentences)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/embeddings.py", line 2338, in _add_embeddings_internal
    all_encoder_layers = self.model(all_input_ids, attention_mask=all_input_masks)[
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/transformers/modeling_bert.py", line 790, in forward
    encoder_attention_mask=encoder_extended_attention_mask,
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/transformers/modeling_bert.py", line 407, in forward
    hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/transformers/modeling_bert.py", line 368, in forward
    self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/transformers/modeling_bert.py", line 314, in forward
    hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/transformers/modeling_bert.py", line 251, in forward
    context_layer = torch.matmul(attention_probs, value_layer)
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 10.73 GiB total capacity; 3.89 GiB already allocated; 9.25 MiB free; 4.27 GiB reserved in total by PyTorch)
2020-04-01 22:06:14,919 epoch 21 - iter 66/220 - loss 1.28301949 - samples/sec: 295.01
------------------------------------
2020-04-01 22:06:10,249 EPOCH 12 done: loss 1.2688 - lr 0.0100
2020-04-01 22:06:14,580 DEV : loss 0.8280799388885498 - score 0.9087
2020-04-01 22:06:14,674 BAD EPOCHS (no improvement): 0
2020-04-01 22:06:16,023 ----------------------------------------------------------------------------------------------------
2020-04-01 22:06:10,362 epoch 12 - iter 387/439 - loss 1.24165886 - samples/sec: 158.64
2020-04-01 22:06:26,280 epoch 21 - iter 88/220 - loss 1.27390667 - samples/sec: 285.73
2020-04-01 22:06:26,116 epoch 12 - iter 430/439 - loss 1.24118202 - samples/sec: 161.17
2020-04-01 22:06:34,641 ---------------------------------------------------------------2020-04-01 22:06:37,084 epoch 21 - iter 110/220 - loss 1.26384816 - samples/sec: 345.72
 - lr 0.0100
2020-04-01 22:06:40,145 epoch 13 - iter 86/439 - loss 1.25045732 - samples/sec: 160.24
2020-04-01 22:06:39,113 DEV : loss 0.8902693390846252 - score 0.9085
2020-04-01 22:06:39,212 BAD EPOCHS (no improvement): 0
2020-04-01 22:06:40,853 ----------------------------------------------------------------------------------------------------
2020-04-01 22:06:48,237 epoch 21 - iter 132/220 - loss 1.26918249 - samples/sec: 321.34
2020-04-01 22:06:49,861 epoch 13 - iter 43/439 - loss 1.16249217 - samples/sec: 152.84
2020-04-01 22:06:58,523 epoch 21 - iter 154/220 - loss 1.26891523 - samples/sec: 343.67
2020-04-01 22:07:09,531 epoch 21 - iter 176/220 - loss 1.27074471 - samples/sec: 344.50
2020-04-01 22:07:21,888 epoch 21 - iter 198/220 - loss 1.25922752 - samples/sec: 322.43
2020-04-01 22:07:32,906 epoch 21 - iter 220/220 - loss 1.25825909 - samples/sec: 351.30
2020-04-01 22:07:36,172 epoch 13 - iter 172/439 - loss 1.17193190 - samples/sec: 153.25
2020-04-01 22:07:38,067 ----------------------------------------------------------------------------------------------------
2020-04-01 22:07:38,068 EPOCH 21 done: loss 1.2583 - lr 0.0100
2020-04-01 22:07:42,062 DEV : loss 0.812343955039978 - score 0.9143
2020-04-01 22:07:42,160 BAD EPOCHS (no improvement): 0
2020-04-01 22:07:43,429 ----------------------------------------------------------------------------------------------------
2020-04-01 22:07:47,274 epoch 22 - iter 22/220 - loss 1.27476255 - samples/sec: 366.54
2020-04-01 22:07:56,420 epoch 22 - iter 44/220 - loss 1.24710869 - samples/sec: 453.49

2020-04-01 22:07:58,110 epoch 13 - iter 301/439 - loss 1.24027689 - samples/sec: 151.87
-----------------------------------
2020-04-01 22:07:59,901 EPOCH 1 done: loss 4.4270 - lr 0.0100
2020-04-01 22:08:05,983 epoch 22 - iter 66/220 - loss 1.27148385 - samples/sec: 408.47

2020-04-01 22:08:15,070 epoch 22 - iter 88/220 - loss 1.24761965 - samples/sec: 421.93

2020-04-01 22:08:24,614 epoch 22 - iter 110/220 - loss 1.25153497 - samples/sec: 428.12
2020-04-01 22:08:32,926 epoch 22 - iter 132/220 - loss 1.26156333 - samples/sec: 448.39
2020-04-01 22:08:34,378 epoch 13 - iter 344/439 - loss 1.17631838 - samples/sec: 150.26
2020-04-01 22:08:41,987 epoch 22 - iter 154/220 - loss 1.26346595 - samples/sec: 457.84
020-04-01 22:08:44,193 epoch 13 - iter 430/439 - loss 1.22756716 - samples/sec: 152.42
2020-04-01 22:08:52,376 ----------------------------------------------------------------------------------------------------
2020-04-01 22:08:52,376 EPOCH 13 done: loss 1.2197 - lr 0.0100
2020-04-01 22:08:56,769 DEV : loss 0.8093656301498413 - score 0.9124
2020-04-01 22:08:56,862 BAD EPOCHS (no improvement): 0
2020-04-01 22:08:49,369 epoch 13 - iter 387/439 - loss 1.18982808 - samples/sec: 153.86
2020-04-01 22:08:51,391 epoch 22 - iter 176/220 - loss 1.25598037 - samples/sec: 439.72
2020-04-01 22:09:00,345 epoch 22 - iter 198/220 - loss 1.25653279 - samples/sec: 463.54
-----------------------------------
2020-04-01 22:09:07,292 epoch 14 - iter 43/439 - loss 1.29339910 - samples/sec: 150.63
2020-04-01 22:09:03,821 epoch 13 - iter 430/439 - loss 1.19220143 - samples/sec: 159.95
2020-04-01 22:09:09,253 epoch 22 - iter 220/220 - loss 1.25261335 - samples/sec: 424.62
2020-04-01 22:09:15,701 ----------------------------------------------------------------------------------------------------
2020-04-01 22:09:15,701 EPOCH 22 done: loss 1.2526 - lr 0.0100
,153 BAD EPOCHS (no improvement): 1
2020-04-01 22:09:16,225 ----------------------------------------------------------------------------------------------------
2020-04-01 22:09:19,518 DEV : loss 0.7809383273124695 - score 0.915
2020-04-01 22:09:192020-04-01 22:09:25,314 epoch 14 - iter 43/439 - loss 1.22928443 - samples/sec: 151.45
-------------------------------------------------------------------------
2020-04-01 22:09:24,499 epoch 23 - iter 22/220 - loss 1.23705516 - samples/sec: 389.13
2020-04-01 22:09:34,086 epoch 23 - iter 44/220 - loss 1.22405094 - samples/sec: 469.21

2020-04-01 22:09:43,698 epoch 23 - iter 66/220 - loss 1.22381181 - samples/sec: 466.26
2020-04-01 22:09:52,563 epoch 23 - iter 88/220 - loss 1.24486364 - samples/sec: 476.95
2020-04-01 22:09:53,266 epoch 14 - iter 172/439 - loss 1.21238441 - samples/sec: 154.18
2020-04-01 22:09:56,269 epoch 14 - iter 129/439 - loss 1.20954016 - samples/sec: 149.85
2020-04-01 22:10:03,662 epoch 23 - iter 110/220 - loss 1.22648735 - samples/sec: 406.50
2020-04-01 22:10:13,001 epoch 23 - iter 132/220 - loss 1.22276646 - samples/sec: 472.96
2020-04-01 22:10:12,495 epoch 14 - iter 172/439 - loss 1.15168946 - samples/sec: 152.14
2020-04-01 22:10:24,092 epoch 23 - iter 154/220 - loss 1.23097563 - samples/sec: 460.98
2020-04-01 22:10:29,653 epoch 14 - iter 215/439 - loss 1.14022610 - samples/sec: 152.67
2020-04-01 22:10:32,602 epoch 23 - iter 176/220 - loss 1.21932058 - samples/sec: 391.33
2020-04-01 22:10:44,384 epoch 1 - iter 172/439 - loss 11.40918848 - samples/sec: 58.42
2020-04-01 22:10:42,438 epoch 14 - iter 301/439 - loss 1.17793888 - samples/sec: 151.42
2020-04-01 22:10:44,440 epoch 14 - iter 258/439 - loss 1.14783317 - samples/sec: 154.12
2020-04-01 22:10:41,708 epoch 23 - iter 198/220 - loss 1.22298828 - samples/sec: 450.24
2020-04-01 22:10:46,900 DEV : loss 1.4441591501235962 - score 0.8476
2020-04-01 22:10:47,325 BAD EPOCHS (no improvement): 0
2020-04-01 22:10:50,646 epoch 23 - iter 220/220 - loss 1.21992702 - samples/sec: 479.41
2020-04-01 22:10:59,497 epoch 14 - iter 301/439 - loss 1.16092566 - samples/sec: 154.82
2020-04-01 22:11:12,290 epoch 14 - iter 387/439 - loss 1.18273397 - samples/sec: 151.20
2020-04-01 22:11:14,357 epoch 14 - iter 344/439 - loss 1.16927582 - samples/sec: 153.21
2020-04-01 22:11:44,009 ----------------------------------------------------------------------------------------------------
train mode resetting embeddings
train mode resetting embeddings
2020-04-01 22:11:44,953 ----------------------------------------------------------------------------------------------------
2020-04-01 22:11:44,953 EPOCH 23 done: loss 1.2199 - lr 0.0100
2020-04-01 22:11:48,829 DEV : loss 0.7689538598060608 - score 0.9154
2020-04-01 22:11:48,929 BAD EPOCHS (no improvement): 0
2020-04-01 22:11:50,110 ----------------------------------------------------------------------------------------------------
2020-04-01 22:11:53,242 epoch 24 - iter 22/220 - loss 1.10904266 - samples/sec: 449.86
2020-04-01 22:12:02,241 epoch 24 - iter 44/220 - loss 1.08069569 - samples/sec: 401.82
,568 BAD EPOCHS (no improvement): 0
2020-04-01 22:11:59,955 ----------------------------------------------------------------------------------------------------
2020-04-01 22:12:00,733 epoch 14 - iter 430/439 - loss 1.16840018 - samples/sec: 159.56
2020-04-01 22:11:58,908 epoch 2 - iter 43/439 - loss 2.12921738 - samples/sec: 92.77
2020-04-01 22:12:11,804 epoch 24 - iter 66/220 - loss 1.14096270 - samples/sec: 412.49
2020-04-01 22:12:08,852 ----------------------------------------------------------------------------------------------------
2020-04-01 22:12:08,853 EPOCH 14 done: loss 1.1672 - lr 0.0100
2020-04-01 22:12:13,323 DEV : loss 0.8250696063041687 - score 0.9136
2020-04-01 22:12:13,423 BAD EPOCHS (no improvement): 0
2020-04-01 22:12:15,231 ----------------------------------------------------------------------------------------------------
2020-04-01 22:12:24,377 epoch 15 - iter 43/439 - loss 1.13792732 - samples/sec: 150.53
2020-04-01 22:12:33,122 epoch 24 - iter 110/220 - loss 1.19225733 - samples/sec: 408.86
2020-04-01 22:12:42,275 epoch 24 - iter 132/220 - loss 1.19230420 - samples/sec: 441.45
2020-04-01 22:12:40,676 epoch 15 - iter 86/439 - loss 1.15322869 - samples/sec: 153.49
2020-04-01 22:12:53,111 epoch 24 - iter 154/220 - loss 1.19651050 - samples/sec: 394.01
2020-04-01 22:13:02,260 epoch 24 - iter 176/220 - loss 1.19697503 - samples/sec: 389.15
2020-04-01 22:12:57,081 epoch 15 - iter 129/439 - loss 1.16518495 - samples/sec: 152.05
2020-04-01 22:13:13,535 epoch 15 - iter 215/439 - loss 1.17024789 - samples/sec: 151.17
2020-04-01 22:13:12,353 epoch 15 - iter 172/439 - loss 1.15201410 - samples/sec: 151.45
2020-04-01 22:13:20,997 epoch 1 - iter 215/439 - loss 10.31587092 - samples/sec: 55.65
2020-04-01 22:13:21,785 epoch 24 - iter 220/220 - loss 1.20325836 - samples/sec: 382.85
2020-04-01 22:13:27,203 -----------------------------------------------------------------2020-04-01 22:13:27,288 epoch 15 - iter 215/439 - loss 1.14100332 - samples/sec: 153.06
 lr 0.0100
2020-04-01 22:13:31,148 DEV : loss 0.762737512588501 - score 0.9168
2020-04-01 22:13:31,245 BAD EPOCHS (no improvement): 0
2020-04-01 22:13:33,025 ----------------------------------------------------------------------------------------------------
2020-04-01 22:13:43,910 epoch 15 - iter 301/439 - loss 1.15890733 - samples/sec: 150.17
2020-04-01 22:13:37,161 epoch 25 - iter 22/220 - loss 1.19434727 - samples/sec: 340.80
2020-04-01 22:13:42,983 epoch 15 - iter 258/439 - loss 1.14835432 - samples/sec: 153.22
2020-04-01 22:14:08,130 epoch 25 - iter 44/220 - loss 1.20751318 - samples/sec: 387.30
22020-04-01 22:14:16,012 epoch 15 - iter 301/439 - loss 1.12702598 - samples/sec: 149.65
2020-04-01 22:14:21,673 epoch 1 - iter 344/439 - loss 6.53348168 - samples/sec: 111.32
2020-04-01 22:14:32,770 epoch 15 - iter 387/439 - loss 1.14949538 - samples/sec: 153.32
2020-04-01 22:14:29,545 epoch 25 - iter 88/220 - loss 1.20017768 - samples/sec: 352.28
2020-04-01 22:14:32,881 epoch 15 - iter 344/439 - loss 1.12972506 - samples/sec: 150.76
2020-04-01 22:14:40,082 epoch 25 - iter 110/220 - loss 1.19943495 - samples/sec: 359.28
2020-04-01 22:14:50,246 epoch 15 - iter 430/439 - loss 1.14769553 - samples/sec: 148.53
2020-04-01 22:14:50,137 epoch 15 - iter 387/439 - loss 1.13331952 - samples/sec: 153.91
2020-04-01 22:15:03,364 epoch 25 - iter 154/220 - loss 1.18852445 - samples/sec: 314.58
-----------------------------------
2020-04-01 22:15:00,237 EPOCH 15 done: loss 1.1462 - lr 0.0100
2020-04-01 22:15:04,691 DEV : loss 0.7799997329711914 - score 0.9131
2020-04-01 22:15:04,789 BAD EPOCHS (no improvement): 0
2020-04-01 22:15:06,155 ----------------------------------------------------------------------------------------------------
2020-04-01 22:15:06,687 epoch 15 - iter 430/439 - loss 1.12610346 - samples/sec: 159.34
2020-04-01 22:15:15,065 epoch 16 - iter 43/439 - loss 1.03281531 - samples/sec: 154.52

2020-04-01 22:15:14,909 ----------------------------------------------------------------------------------------------------
2020-04-01 22:15:14,909 EPOCH 15 done: loss 1.1253 - lr 0.0100
2020-04-01 22:15:26,330 epoch 25 - iter 198/220 - loss 1.17661392 - samples/sec: 310.16
2020-04-01 22:15:19,366 DEV : loss 0.7981631755828857 - score 0.9138
2020-04-01 22:15:19,465 BAD EPOCHS (no improvement): 0
2020-04-01 22:15:21,173 ----------------------------------------------------------------------------------------------------
2020-04-01 22:15:37,067 epoch 25 - iter 220/220 - loss 1.18250806 - samples/sec: 323.58
2020-04-01 22:15:30,167 epoch 16 - iter 43/439 - loss 1.12934425 - samples/sec: 153.08
2020-04-01 22:15:43,903 -----------------------------------------------------------------2020-04-01 22:15:45,512 epoch 16 - iter 86/439 - loss 1.10836682 - samples/sec: 151.81
- lr 0.0100
2020-04-01 22:15:47,830 DEV : loss 0.7382555603981018 - score 0.9191
2020-04-01 22:15:47,926 BAD EPOCHS (no improvement): 0
2020-04-01 22:15:49,219 ----------------------------------------------------------------------------------------------------
2020-04-01 22:15:53,457 epoch 26 - iter 22/220 - loss 1.14281265 - samples/sec: 332.61
2020-04-01 22:16:03,326 epoch 26 - iter 44/220 - loss 1.17764898 - samples/sec: 360.91

2020-04-01 22:16:00,587 epoch 16 - iter 129/439 - loss 1.06566873 - samples/sec: 154.07
2020-04-01 22:16:16,288 epoch 26 - iter 66/220 - loss 1.18371614 - samples/sec: 291.87

2020-04-01 22:16:26,851 epoch 26 - iter 88/220 - loss 1.18887014 - samples/sec: 321.97
2020-04-01 22:16:20,199 epoch 16 - iter 215/439 - loss 1.08430852 - samples/sec: 147.53
2020-04-01 22:16:38,111 epoch 26 - iter 110/220 - loss 1.17754840 - samples/sec: 303.08
2020-04-01 22:16:32,644 epoch 16 - iter 215/439 - loss 1.08170477 - samples/sec: 152.67
2020-04-01 22:16:48,298 epoch 16 - iter 258/439 - loss 1.07399617 - samples/sec: 151.35
2020-04-01 22:16:48,926 epoch 26 - iter 132/220 - loss 1.16608381 - samples/sec: 326.50
2020-04-01 22:17:00,335 epoch 26 - iter 154/220 - loss 1.17133600 - samples/sec: 336.70
2020-04-01 22:17:04,401 epoch 16 - iter 301/439 - loss 1.08341312 - samples/sec: 155.31
2020-04-01 22:17:11,420 epoch 26 - iter 176/220 - loss 1.16430360 - samples/sec: 302.69
2020-04-01 22:17:22,933 epoch 26 - iter 198/220 - loss 1.16117039 - samples/sec: 299.91
2020-04-01 22:17:20,109 epoch 16 - iter 344/439 - loss 1.07319430 - samples/sec: 153.79
2020-04-01 22:17:34,902 epoch 26 - iter 220/220 - loss 1.16634524 - samples/sec: 310.38
2020-04-01 22:17:41,342 ----------------------------------------------------------------------------------------------------
2020-04-01 22:17:41,342 EPOCH 26 done: loss 1.1663 - lr 0.0100
2020-04-01 22:17:45,172 DEV : loss 0.728958010673523 - score 0.9196
2020-04-01 22:17:45,269 BAD EPOCHS (no improvement): 0
2020-04-01 22:17:46,504 ----------------------------------------------------------------------------------------------------
2020-04-01 22:17:53,468 epoch 2 - iter 86/439 - loss 2.04580265 - samples/sec: 90.94
2020-04-01 22:17:50,017 epoch 27 - iter 22/220 - loss 1.14123704 - samples/sec: 401.05
------------------------------------
2020-04-01 22:17:48,514 EPOCH 16 done: loss 1.1161 - lr 0.0100
2020-04-01 22:17:52,970 DEV : loss 0.7506149411201477 - score 0.9149
2020-04-01 22:17:53,066 BAD EPOCHS (no improvement): 0
2020-04-01 22:17:54,508 ----------------------------------------------------------------------------------------------------
2020-04-01 22:17:51,074 epoch 16 - iter 430/439 - loss 1.08088698 - samples/sec: 161.00
2020-04-01 22:18:00,524 epoch 27 - iter 44/220 - loss 1.16795203 - samples/sec: 322.69
020-04-01 22:18:03,535 epoch 17 - iter 43/439 - loss 1.03692387 - samples/sec: 152.52
2020-04-01 22:17:59,342 ----------------------------------------------------------------------------------------------------
2020-04-01 22:17:59,343 EPOCH 16 done: loss 1.0855 - lr 0.0100
2020-04-01 22:18:03,783 DEV : loss 0.7930755615234375 - score 0.9137
2020-04-01 22:18:03,879 BAD EPOCHS (no improvement): 1
2020-04-01 22:18:03,938 ----------------------------------------------------------------------------------------------------
2020-04-01 22:18:10,791 epoch 27 - iter 66/220 - loss 1.15185566 - samples/sec: 328.04
2020-04-01 22:18:21,104 epoch 27 - iter 88/220 - loss 1.17044806 - samples/sec: 332.55
2020-04-01 22:18:18,757 epoch 17 - iter 86/439 - loss 1.11092610 - samples/sec: 154.81
2020-04-01 22:18:27,979 epoch 17 - iter 86/439 - loss 1.08107637 - samples/sec: 150.65
2020-04-01 22:18:31,636 epoch 27 - iter 110/220 - loss 1.14942252 - samples/sec: 317.36
2020-04-01 22:18:42,353 epoch 17 - iter 129/439 - loss 1.05462194 - samples/sec: 160.53
2020-04-01 22:18:42,081 epoch 27 - iter 132/220 - loss 1.14550234 - samples/sec: 352.71
2020-04-01 22:18:49,848 epoch 17 - iter 172/439 - loss 1.06078192 - samples/sec: 151.87
2020-04-01 22:18:58,065 epoch 17 - iter 172/439 - loss 1.06079543 - samples/sec: 153.37
2020-04-01 22:18:52,961 epoch 27 - iter 154/220 - loss 1.13403872 - samples/sec: 338.57
2020-04-01 22:19:04,660 epoch 27 - iter 176/220 - loss 1.14038060 - samples/sec: 305.84
2020-04-01 22:19:15,864 epoch 27 - iter 198/220 - loss 1.13235979 - samples/sec: 429.82
2020-04-01 22:19:26,214 epoch 27 - iter 220/220 - loss 1.13792513 - samples/sec: 439.97
2020-04-01 22:19:33,736 ----------------------------------------------------------------------------------------------------
2020-04-01 22:19:33,736 EPOCH 27 done: loss 1.1379 - lr 0.0100
2020-04-01 22:19:37,638 DEV : loss 0.7447772026062012 - score 0.9187
2020-04-01 22:19:37,737 BAD EPOCHS (no improvement): 1
2020-04-01 22:19:37,761 ----------------------------------------------------------------------------------------------------
2020-04-01 22:19:40,684 epoch 28 - iter 22/220 - loss 1.15745943 - samples/sec: 481.94

2020-04-01 22:19:50,489 epoch 28 - iter 44/220 - loss 1.13081054 - samples/sec: 419.07
2020-04-01 22:19:53,868 epoch 17 - iter 344/439 - loss 1.07991062 - samples/sec: 153.45
2020-04-01 22:20:01,711 epoch 28 - iter 66/220 - loss 1.15043766 - samples/sec: 315.88

2020-04-01 22:20:11,899 epoch 28 - iter 88/220 - loss 1.18288908 - samples/sec: 344.57

-----------------------------------
2020-04-01 22:20:11,698 EPOCH 1 done: loss 5.9095 - lr 0.0100
2020-04-01 22:20:16,286 epoch 17 - iter 387/439 - loss 1.07470012 - samples/sec: 153.54
2020-04-01 22:20:21,810 epoch 28 - iter 110/220 - loss 1.15662593 - samples/sec: 323.40
20-04-01 22:20:26,977 DEV : loss 2.5116119384765625 - score 0.7575
2020-04-01 22:20:27,073 BAD EPOCHS (no improvement): 0
2020-04-01 22:20:24,337 epoch 17 - iter 430/439 - loss 1.08319063 - samples/sec: 154.18
2020-04-01 22:20:32,557 epoch 28 - iter 132/220 - loss 1.14379884 - samples/sec: 345.19
-----------------------------------
2020-04-01 22:20:33,081 EPOCH 17 done: loss 1.0813 - lr 0.0100
2020-04-01 22:20:37,526 DEV : loss 0.723701000213623 - score 0.9166
2020-04-01 22:20:37,622 BAD EPOCHS (no improvement): 0
2020-04-01 22:20:30,992 epoch 17 - iter 430/439 - loss 1.07027200 - samples/sec: 151.86
2020-04-01 22:20:48,484 epoch 28 - iter 154/220 - loss 1.14549419 - samples/sec: 315.55
20-04-01 22:20:39,043 ----------------------------------------------------------------------------------------------------
2020-04-01 22:20:47,725 epoch 18 - iter 43/439 - loss 0.95497301 - samples/sec: 158.57
2020-04-01 22:20:45,504 ----------------------------------------------------------------------------------------------------
2020-04-01 22:20:45,505 EPOCH 17 done: loss 1.0688 - lr 0.0100
2020-04-01 22:20:49,921 DEV : loss 0.7602602243423462 - score 0.9174
2020-04-01 22:20:50,018 BAD EPOCHS (no improvement): 0
2020-04-01 22:20:51,618 ----------------------------------------------------------------------------------------------------
2020-04-01 22:21:01,502 epoch 28 - iter 176/220 - loss 1.13519138 - samples/sec: 304.69
020-04-01 22:21:00,858 epoch 18 - iter 43/439 - loss 1.06285469 - samples/sec: 149.00
2020-04-01 22:21:14,949 epoch 28 - iter 198/220 - loss 1.12594235 - samples/sec: 295.13
2020-04-01 22:21:27,066 epoch 28 - iter 220/220 - loss 1.12774398 - samples/sec: 289.91
2020-04-01 22:21:37,976 epoch 18 - iter 172/439 - loss 1.03970711 - samples/sec: 151.32
2020-04-01 22:21:34,405 epoch 18 - iter 129/439 - loss 1.05191842 - samples/sec: 151.25
2020-04-01 22:21:33,857 ----------------------------------------------------------------------------------------------------
2020-04-01 22:21:33,858 EPOCH 28 done: loss 1.1277 - lr 0.0100
2020-04-01 22:21:37,781 DEV : loss 0.7154833674430847 - score 0.918
2020-04-01 22:21:37,878 BAD EPOCHS (no improvement): 2
2020-04-01 22:21:37,913 ----------------------------------------------------------------------------------------------------
2020-04-01 22:21:41,440 epoch 29 - iter 22/220 - loss 1.09705371 - samples/sec: 399.48
2020-04-01 22:21:54,022 epoch 18 - iter 215/439 - loss 1.03810794 - samples/sec: 150.25
2020-04-01 22:21:49,829 epoch 18 - iter 172/439 - loss 1.04169247 - samples/sec: 156.78
2020-04-01 22:21:53,006 epoch 29 - iter 44/220 - loss 1.10996347 - samples/sec: 338.16
2020-04-01 22:22:04,812 epoch 29 - iter 66/220 - loss 1.13369675 - samples/sec: 313.61

2020-04-01 22:22:15,801 epoch 29 - iter 88/220 - loss 1.14189773 - samples/sec: 319.92

2020-04-01 22:22:27,260 epoch 29 - iter 110/220 - loss 1.13832104 - samples/sec: 297.02
2020-04-01 22:22:21,911 epoch 18 - iter 258/439 - loss 1.04241772 - samples/sec: 152.64
2020-04-01 22:22:38,920 epoch 29 - iter 132/220 - loss 1.13458602 - samples/sec: 323.81
2020-04-01 22:22:49,943 epoch 29 - iter 154/220 - loss 1.13429068 - samples/sec: 311.89
20-04-01 22:22:44,021 epoch 2 - iter 86/439 - loss 3.29245018 - samples/sec: 187.65
2020-04-01 22:22:41,996 epoch 18 - iter 344/439 - loss 1.05389685 - samples/sec: 149.66
2020-04-01 22:23:00,008 epoch 29 - iter 176/220 - loss 1.12500238 - samples/sec: 479.79
2020-04-01 22:22:52,306 epoch 18 - iter 344/439 - loss 1.03402100 - samples/sec: 156.00
2020-04-01 22:23:09,726 epoch 29 - iter 198/220 - loss 1.10728306 - samples/sec: 438.06
2020-04-01 22:23:13,120 epoch 18 - iter 430/439 - loss 1.06311978 - samples/sec: 153.75
2020-04-01 22:23:20,348 epoch 29 - iter 220/220 - loss 1.10850884 - samples/sec: 421.08
2020-04-01 22:23:27,925 ----------------------------------------------------------------------------------------------------
2020-04-01 22:23:27,925 EPOCH 29 done: loss 1.1085 - lr 0.0100
,765 BAD EPOCHS (no improvement): 0
2020-04-01 22:23:27,197 ----------------------------------------------------------------------------------------------------
2020-04-01 22:23:24,190 epoch 18 - iter 430/439 - loss 1.03586582 - samples/sec: 152.89
2020-04-01 22:23:35,987 epoch 19 - iter 43/439 - loss 0.97021200 - samples/sec: 156.622020-04-01 22:23:31,749 DEV : loss 0.7203491926193237 - score 0.9213
2020-04-01 22:23:31,846 BAD EPOCHS (no improvement): 0
2020-04-01 22:23:33,091 ----------------------------------------------------------------------------------------------------
2020-04-01 22:23:36,762 epoch 30 - iter 22/220 - loss 1.08302976 - samples/sec: 383.98
2020-04-01 22:23:48,927 epoch 30 - iter 44/220 - loss 1.08047762 - samples/sec: 299.35
------------------------------------
2020-04-01 22:23:49,065 epoch 19 - iter 43/439 - loss 1.06536044 - samples/sec: 150.29
2020-04-01 22:23:50,444 epoch 3 - iter 132/220 - loss 1.98598351 - samples/sec: 113.67
2020-04-01 22:23:53,107 epoch 19 - iter 86/439 - loss 0.95865798 - samples/sec: 149.33
2020-04-01 22:24:01,931 epoch 30 - iter 66/220 - loss 1.09548627 - samples/sec: 318.59
2020-04-01 22:24:10,667 epoch 19 - iter 129/439 - loss 1.00799338 - samples/sec: 151.89
2020-04-01 22:24:14,474 epoch 30 - iter 88/220 - loss 1.08276680 - samples/sec: 316.89
2020-04-01 22:24:26,946 epoch 30 - iter 110/220 - loss 1.08179081 - samples/sec: 284.47
2020-04-01 22:24:23,169 epoch 19 - iter 129/439 - loss 1.04402065 - samples/sec: 152.19
2020-04-01 22:24:38,769 epoch 30 - iter 132/220 - loss 1.08170430 - samples/sec: 362.05
2020-04-01 22:24:50,519 epoch 30 - iter 154/220 - loss 1.09490540 - samples/sec: 292.80
020-04-01 22:24:43,816 epoch 19 - iter 215/439 - loss 0.99913699 - samples/sec: 151.81
2020-04-01 22:24:59,782 epoch 30 - iter 176/220 - loss 1.10020892 - samples/sec: 413.00
2020-04-01 22:24:55,174 epoch 19 - iter 215/439 - loss 1.04197480 - samples/sec: 150.69
2020-04-01 22:25:10,756 epoch 30 - iter 198/220 - loss 1.09984390 - samples/sec: 311.04
2020-04-01 22:25:14,704 epoch 19 - iter 301/439 - loss 1.01095544 - samples/sec: 151.75
2020-04-01 22:25:10,740 epoch 19 - iter 258/439 - loss 1.02451311 - samples/sec: 153.42
2020-04-01 22:25:23,088 epoch 30 - iter 220/220 - loss 1.09855406 - samples/sec: 331.46
2020-04-01 22:25:29,751 ----------------------------------------------------------------------------------------------------
2020-04-01 22:25:29,751 EPOCH 30 done: loss 1.0986 - lr 0.0100
2020-04-01 22:25:31,614 epoch 19 - iter 344/439 - loss 1.03086114 - samples/sec: 152.99
2020-04-01 22:25:33,693 DEV : loss 0.7148537039756775 - score 0.9209
2020-04-01 22:25:33,791 BAD EPOCHS (no improvement): 1
2020-04-01 22:25:33,844 ----------------------------------------------------------------------------------------------------
2020-04-01 22:25:37,730 epoch 31 - iter 22/220 - loss 1.08581254 - samples/sec: 362.61
2020-04-01 22:25:48,023 epoch 31 - iter 44/220 - loss 1.07129381 - samples/sec: 311.68

2020-04-01 22:25:43,376 epoch 19 - iter 344/439 - loss 1.00248984 - samples/sec: 152.46
2020-04-01 22:25:59,460 epoch 31 - iter 66/220 - loss 1.06353145 - samples/sec: 317.66

2020-04-01 22:26:10,243 epoch 31 - iter 88/220 - loss 1.06096945 - samples/sec: 316.02

2020-04-01 22:26:20,554 epoch 31 - iter 110/220 - loss 1.07504185 - samples/sec: 322.74
-----------------------------------
2020-04-01 22:26:11,584 EPOCH 19 done: loss 1.0309 - lr 0.0100
2020-04-01 22:26:16,053 DEV : loss 0.700398325920105 - score 0.9199
2020-04-01 22:26:16,149 BAD EPOCHS (no improvement): 0
2020-04-01 22:26:17,599 ----------------------------------------------------------------------------------------------------
2020-04-01 22:26:13,779 epoch 19 - iter 430/439 - loss 1.00498363 - samples/sec: 159.47
2020-04-01 22:26:26,544 epoch 20 - iter 43/439 - loss 1.14451951 - samples/sec: 153.91
2020-04-01 22:26:22,387 ----------------------------------------------------------------------------------------------------
2020-04-01 22:26:22,387 EPOCH 19 done: loss 1.0070 - lr 0.0100
2020-04-01 22:26:26,814 DEV : loss 0.7289733290672302 - score 0.9205
2020-04-01 22:26:26,909 BAD EPOCHS (no improvement): 0
2020-04-01 22:26:28,496 ----------------------------------------------------------------------------------------------------
2020-04-01 22:26:31,298 epoch 31 - iter 132/220 - loss 1.07003295 - samples/sec: 283.40
2020-04-01 22:26:41,755 epoch 31 - iter 154/220 - loss 1.07875810 - samples/sec: 303.16
2020-04-01 22:26:52,714 epoch 31 - iter 176/220 - loss 1.07929085 - samples/sec: 309.89
-----------------------------------
2020-04-01 22:26:50,387 EPOCH 1 done: loss 7.2311 - lr 0.0100
2020-04-01 22:26:52,769 epoch 20 - iter 86/439 - loss 0.93482424 - samples/sec: 154.08
2020-04-01 22:27:07,849 epoch 20 - iter 129/439 - loss 0.92745090 - samples/sec: 148.95
ent call last):
  File "bagging_train.py", line 63, in <module>
    max_epochs=150)
  File "/hdd2/qingpeng/code/mix_ner_flair/ensemble_trainer.py", line 426, in train
    embedding_storage_mode=embeddings_storage_mode,
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py", line 412, in evaluate
    features = self.forward(batch)
  File "/home/qingpeng/anaconda3/envs/cuda100/lib/python3.6/site-packages/flair/models/sequence_tagger_model.py", line 526, in forward
    self.embeddings.embedding_length,
RuntimeError: shape '[32, 41, 768]' is invalid for input of size 3142656
2020-04-01 22:27:03,072 epoch 31 - iter 198/220 - loss 1.06556765 - samples/sec: 310.24
2020-04-01 22:27:13,044 epoch 20 - iter 172/439 - loss 0.99383828 - samples/sec: 151.25
2020-04-01 22:27:14,059 epoch 31 - iter 220/220 - loss 1.06209479 - samples/sec: 269.74
2020-04-01 22:27:20,795 ----------------------------------------------------------------------------------------------------
2020-04-01 22:27:20,796 EPOCH 31 done: loss 1.0621 - lr 0.0100
2020-04-01 22:27:28,949 epoch 20 - iter 215/439 - loss 0.98975640 - samples/sec: 150.06
486 BAD EPOCHS (no improvement): 0
2020-04-01 22:27:26,801 ----------------------------------------------------------------------------------------------------
2020-04-01 22:27:31,342 epoch 32 - iter 22/220 - loss 1.04321940 - samples/sec: 310.31
2020-04-01 22:27:22,703 epoch 20 - iter 172/439 - loss 0.96502139 - samples/sec: 157.29
2020-04-01 22:27:38,189 epoch 20 - iter 215/439 - loss 0.97906363 - samples/sec: 153.18
2020-04-01 22:27:45,187 epoch 3 - iter 154/220 - loss 1.97281688 - samples/sec: 123.35
2020-04-01 22:27:44,488 epoch 20 - iter 258/439 - loss 0.99285302 - samples/sec: 151.90
2020-04-01 22:27:59,695 epoch 20 - iter 301/439 - loss 1.00421022 - samples/sec: 150.40
2020-04-01 22:27:52,989 epoch 20 - iter 258/439 - loss 0.97184248 - samples/sec: 151.84
2020-04-01 22:28:08,506 epoch 20 - iter 301/439 - loss 0.98756474 - samples/sec: 155.55
2020-04-01 22:28:15,272 epoch 20 - iter 344/439 - loss 1.01293292 - samples/sec: 155.70
2020-04-01 22:28:14,946 epoch 32 - iter 110/220 - loss 1.05666693 - samples/sec: 321.79
2020-04-01 22:28:25,717 epoch 32 - iter 132/220 - loss 1.06326907 - samples/sec: 304.03
2020-04-01 22:28:23,525 epoch 20 - iter 344/439 - loss 0.98948573 - samples/sec: 152.71
2020-04-01 22:28:38,373 epoch 20 - iter 387/439 - loss 0.99123881 - samples/sec: 151.32
020-04-01 22:28:35,752 epoch 32 - iter 154/220 - loss 1.05957161 - samples/sec: 339.48
2020-04-01 22:28:45,240 epoch 20 - iter 430/439 - loss 1.01127324 - samples/sec: 153.70
2020-04-01 22:28:46,398 epoch 32 - iter 176/220 - loss 1.06236655 - samples/sec: 291.75
2020-04-01 22:28:53,745 ----------------------------------------------------------------------------------------------------
2020-04-01 22:28:53,745 EPOCH 20 done: loss 1.0122 - lr 0.0100
2020-04-01 22:28:58,206 DEV : loss 0.6987057328224182 - score 0.92
2020-04-01 22:28:58,302 BAD EPOCHS (no improvement): 0
2020-04-01 22:28:59,733 ----------------------------------------------------------------------------------------------------
2020-04-01 22:28:53,847 epoch 20 - iter 430/439 - loss 0.99925692 - samples/sec: 151.18
2020-04-01 22:29:07,573 epoch 32 - iter 220/220 - loss 1.06139905 - samples/sec: 416.662020-04-01 22:29:03,309 ----------------------------------------------------------------------------------------------------
2020-04-01 22:29:03,310 EPOCH 20 done: loss 1.0033 - lr 0.0100
2020-04-01 22:29:08,436 DEV : loss 0.723555862903595 - score 0.9222
2020-04-01 22:29:08,535 BAD EPOCHS (no improvement): 0
2020-04-01 22:29:10,217 ----------------------------------------------------------------------------------------------------
2020-04-01 22:29:15,060 epoch 2 - iter 172/439 - loss 2.06010061 - samples/sec: 84.46
2020-04-01 22:29:18,958 epoch 21 - iter 43/439 - loss 0.93098529 - samples/sec: 157.51
------------------------------------
2020-04-01 22:29:14,507 EPOCH 32 done: loss 1.0614 - lr 0.0100
2020-04-01 22:29:18,388 DEV : loss 0.6845597624778748 - score 0.9244
2020-04-01 22:29:18,487 BAD EPOCHS (no improvement): 0
2020-04-01 22:29:19,697 ----------------------------------------------------------------------------------------------------
2020-04-01 22:29:24,447 epoch 33 - iter 22/220 - loss 1.03587099 - samples/sec: 296.71
